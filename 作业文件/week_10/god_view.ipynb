{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b_0</th>\n",
       "      <th>b_1</th>\n",
       "      <th>b_2</th>\n",
       "      <th>b_3</th>\n",
       "      <th>b_4</th>\n",
       "      <th>b_5</th>\n",
       "      <th>b_6</th>\n",
       "      <th>b_7</th>\n",
       "      <th>b_8</th>\n",
       "      <th>b_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b_0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-0.017600</td>\n",
       "      <td>-0.018667</td>\n",
       "      <td>-1.855553e-17</td>\n",
       "      <td>-0.010133</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-0.010133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_1</th>\n",
       "      <td>3.733333e-03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017067</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>-0.013333</td>\n",
       "      <td>-2.666667e-03</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>-0.015467</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_2</th>\n",
       "      <td>-3.200000e-03</td>\n",
       "      <td>0.017067</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.020267</td>\n",
       "      <td>-8.000000e-03</td>\n",
       "      <td>0.012267</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>-0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_3</th>\n",
       "      <td>-1.760000e-02</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>1.066667e-03</td>\n",
       "      <td>0.021867</td>\n",
       "      <td>0.010133</td>\n",
       "      <td>-0.012800</td>\n",
       "      <td>0.005867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_4</th>\n",
       "      <td>-1.866667e-02</td>\n",
       "      <td>-0.013333</td>\n",
       "      <td>0.020267</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.600000e-03</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>-0.008000</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.023467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_5</th>\n",
       "      <td>-1.855553e-17</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-0.008000</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.010133</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-0.018667</td>\n",
       "      <td>-0.017067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_6</th>\n",
       "      <td>-1.013333e-02</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.012267</td>\n",
       "      <td>0.021867</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>-1.013333e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>-0.015467</td>\n",
       "      <td>-0.001067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_7</th>\n",
       "      <td>1.866667e-02</td>\n",
       "      <td>-0.015467</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.010133</td>\n",
       "      <td>-0.008000</td>\n",
       "      <td>-3.200000e-03</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008000</td>\n",
       "      <td>-0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_8</th>\n",
       "      <td>-3.200000e-03</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>-0.012800</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>-1.866667e-02</td>\n",
       "      <td>-0.015467</td>\n",
       "      <td>-0.008000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_9</th>\n",
       "      <td>-1.013333e-02</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>0.023467</td>\n",
       "      <td>-1.706667e-02</td>\n",
       "      <td>-0.001067</td>\n",
       "      <td>-0.011200</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              b_0       b_1       b_2       b_3       b_4           b_5  \\\n",
       "b_0  1.000000e+00  0.003733 -0.003200 -0.017600 -0.018667 -1.855553e-17   \n",
       "b_1  3.733333e-03  1.000000  0.017067  0.001600 -0.013333 -2.666667e-03   \n",
       "b_2 -3.200000e-03  0.017067  1.000000  0.013333  0.020267 -8.000000e-03   \n",
       "b_3 -1.760000e-02  0.001600  0.013333  1.000000  0.002133  1.066667e-03   \n",
       "b_4 -1.866667e-02 -0.013333  0.020267  0.002133  1.000000 -1.600000e-03   \n",
       "b_5 -1.855553e-17 -0.002667 -0.008000  0.001067 -0.001600  1.000000e+00   \n",
       "b_6 -1.013333e-02  0.001600  0.012267  0.021867  0.006400 -1.013333e-02   \n",
       "b_7  1.866667e-02 -0.015467  0.018667  0.010133 -0.008000 -3.200000e-03   \n",
       "b_8 -3.200000e-03 -0.004267  0.011200 -0.012800  0.005333 -1.866667e-02   \n",
       "b_9 -1.013333e-02  0.003200 -0.001600  0.005867  0.023467 -1.706667e-02   \n",
       "\n",
       "          b_6       b_7       b_8       b_9  \n",
       "b_0 -0.010133  0.018667 -0.003200 -0.010133  \n",
       "b_1  0.001600 -0.015467 -0.004267  0.003200  \n",
       "b_2  0.012267  0.018667  0.011200 -0.001600  \n",
       "b_3  0.021867  0.010133 -0.012800  0.005867  \n",
       "b_4  0.006400 -0.008000  0.005333  0.023467  \n",
       "b_5 -0.010133 -0.003200 -0.018667 -0.017067  \n",
       "b_6  1.000000  0.006400 -0.015467 -0.001067  \n",
       "b_7  0.006400  1.000000 -0.008000 -0.011200  \n",
       "b_8 -0.015467 -0.008000  1.000000  0.001067  \n",
       "b_9 -0.001067 -0.011200  0.001067  1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 0-1相关的情况\n",
    "random_1v4 = [1] * 2500 + [0] * 7500\n",
    "random.seed(211113)\n",
    "\n",
    "df_random_corr = pd.DataFrame()\n",
    "for ii in range(10):\n",
    "    random.shuffle(random_1v4)\n",
    "    df_random_corr[f'b_{ii}'] = random_1v4.copy()\n",
    "\n",
    "df_random_corr.corr()\n",
    "# 结论：随机情况确实是基本不相关的\n",
    "# 如果是这个量级的相关性，也基本可以认为是没有相关信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_0</th>\n",
       "      <th>n_1</th>\n",
       "      <th>n_2</th>\n",
       "      <th>n_3</th>\n",
       "      <th>n_4</th>\n",
       "      <th>n_5</th>\n",
       "      <th>n_6</th>\n",
       "      <th>n_7</th>\n",
       "      <th>n_8</th>\n",
       "      <th>n_9</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>-0.007124</td>\n",
       "      <td>-0.016567</td>\n",
       "      <td>-0.011230</td>\n",
       "      <td>-0.002509</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.012568</td>\n",
       "      <td>-0.004166</td>\n",
       "      <td>-0.009140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_1</th>\n",
       "      <td>0.005477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009014</td>\n",
       "      <td>-0.014333</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>-0.003347</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>-0.001885</td>\n",
       "      <td>-0.016554</td>\n",
       "      <td>-0.002289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_2</th>\n",
       "      <td>-0.007124</td>\n",
       "      <td>-0.009014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>0.014015</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>-0.010459</td>\n",
       "      <td>-0.019145</td>\n",
       "      <td>-0.003916</td>\n",
       "      <td>0.006524</td>\n",
       "      <td>-0.010256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_3</th>\n",
       "      <td>-0.016567</td>\n",
       "      <td>-0.014333</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004712</td>\n",
       "      <td>-0.005638</td>\n",
       "      <td>0.011394</td>\n",
       "      <td>0.018079</td>\n",
       "      <td>-0.008359</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.004747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_4</th>\n",
       "      <td>-0.011230</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.014015</td>\n",
       "      <td>-0.004712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002204</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>0.019938</td>\n",
       "      <td>0.011349</td>\n",
       "      <td>-0.028646</td>\n",
       "      <td>0.023931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_5</th>\n",
       "      <td>-0.002509</td>\n",
       "      <td>-0.003347</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>-0.005638</td>\n",
       "      <td>-0.002204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015864</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>-0.014174</td>\n",
       "      <td>-0.001923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_6</th>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>-0.010459</td>\n",
       "      <td>0.011394</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>0.015864</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.002641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_7</th>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>-0.019145</td>\n",
       "      <td>0.018079</td>\n",
       "      <td>0.019938</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>-0.006209</td>\n",
       "      <td>-0.004090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_8</th>\n",
       "      <td>0.012568</td>\n",
       "      <td>-0.001885</td>\n",
       "      <td>-0.003916</td>\n",
       "      <td>-0.008359</td>\n",
       "      <td>0.011349</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001722</td>\n",
       "      <td>-0.007637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_9</th>\n",
       "      <td>-0.004166</td>\n",
       "      <td>-0.016554</td>\n",
       "      <td>0.006524</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.028646</td>\n",
       "      <td>-0.014174</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.006209</td>\n",
       "      <td>-0.001722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>-0.009140</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>-0.010256</td>\n",
       "      <td>-0.004747</td>\n",
       "      <td>0.023931</td>\n",
       "      <td>-0.001923</td>\n",
       "      <td>-0.002641</td>\n",
       "      <td>-0.004090</td>\n",
       "      <td>-0.007637</td>\n",
       "      <td>-0.002930</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            n_0       n_1       n_2       n_3       n_4       n_5       n_6  \\\n",
       "n_0    1.000000  0.005477 -0.007124 -0.016567 -0.011230 -0.002509  0.011550   \n",
       "n_1    0.005477  1.000000 -0.009014 -0.014333  0.002126 -0.003347  0.012255   \n",
       "n_2   -0.007124 -0.009014  1.000000  0.006036  0.014015  0.004081 -0.010459   \n",
       "n_3   -0.016567 -0.014333  0.006036  1.000000 -0.004712 -0.005638  0.011394   \n",
       "n_4   -0.011230  0.002126  0.014015 -0.004712  1.000000 -0.002204 -0.007450   \n",
       "n_5   -0.002509 -0.003347  0.004081 -0.005638 -0.002204  1.000000  0.015864   \n",
       "n_6    0.011550  0.012255 -0.010459  0.011394 -0.007450  0.015864  1.000000   \n",
       "n_7    0.013138  0.005137 -0.019145  0.018079  0.019938  0.006581  0.000073   \n",
       "n_8    0.012568 -0.001885 -0.003916 -0.008359  0.011349  0.002938  0.007481   \n",
       "n_9   -0.004166 -0.016554  0.006524  0.000060 -0.028646 -0.014174  0.000141   \n",
       "label -0.009140 -0.002289 -0.010256 -0.004747  0.023931 -0.001923 -0.002641   \n",
       "\n",
       "            n_7       n_8       n_9     label  \n",
       "n_0    0.013138  0.012568 -0.004166 -0.009140  \n",
       "n_1    0.005137 -0.001885 -0.016554 -0.002289  \n",
       "n_2   -0.019145 -0.003916  0.006524 -0.010256  \n",
       "n_3    0.018079 -0.008359  0.000060 -0.004747  \n",
       "n_4    0.019938  0.011349 -0.028646  0.023931  \n",
       "n_5    0.006581  0.002938 -0.014174 -0.001923  \n",
       "n_6    0.000073  0.007481  0.000141 -0.002641  \n",
       "n_7    1.000000  0.002375 -0.006209 -0.004090  \n",
       "n_8    0.002375  1.000000 -0.001722 -0.007637  \n",
       "n_9   -0.006209 -0.001722  1.000000 -0.002930  \n",
       "label -0.004090 -0.007637 -0.002930  1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 前面都是正态分布，后面是0-1的情况\n",
    "random_1v4 = [1] * 2500 + [0] * 7500\n",
    "random.seed(211113)\n",
    "\n",
    "df_random_corr = pd.DataFrame()\n",
    "for ii in range(10):\n",
    "    df_random_corr[f'n_{ii}'] = np.random.normal(size=[10000])\n",
    "\n",
    "random.shuffle(random_1v4)\n",
    "df_random_corr['label'] = random_1v4.copy()\n",
    "\n",
    "df_random_corr.corr()\n",
    "# 与上个框的结论一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-f0530d7c61ae>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_random_corr['b1c3'][list(range(0, 10000, 4))] = df_random_corr['base'][list(range(0, 10000, 4))]\n",
      "<ipython-input-4-f0530d7c61ae>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_random_corr['b1c2'][list(range(0, 10000, 3))] = df_random_corr['base'][list(range(0, 10000, 3))]\n",
      "<ipython-input-4-f0530d7c61ae>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_random_corr['b1c1'][list(range(0, 10000, 2))] = df_random_corr['base'][list(range(0, 10000, 2))]\n",
      "<ipython-input-4-f0530d7c61ae>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_random_corr['b3c1'][list(range(0, 10000, 4))] = df_random_corr['comp'][list(range(0, 10000, 4))]\n",
      "<ipython-input-4-f0530d7c61ae>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_random_corr['b2c1'][list(range(0, 10000, 3))] = df_random_corr['comp'][list(range(0, 10000, 3))]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base</th>\n",
       "      <th>base_01</th>\n",
       "      <th>m25p3</th>\n",
       "      <th>square</th>\n",
       "      <th>exp</th>\n",
       "      <th>comp</th>\n",
       "      <th>b1c3</th>\n",
       "      <th>b1c2</th>\n",
       "      <th>b1c1</th>\n",
       "      <th>b3c1</th>\n",
       "      <th>b2c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034218</td>\n",
       "      <td>0.749334</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>0.263940</td>\n",
       "      <td>0.329302</td>\n",
       "      <td>0.510820</td>\n",
       "      <td>0.732278</td>\n",
       "      <td>0.665082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_01</th>\n",
       "      <td>0.798438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798438</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.500323</td>\n",
       "      <td>0.009729</td>\n",
       "      <td>0.209910</td>\n",
       "      <td>0.271262</td>\n",
       "      <td>0.406129</td>\n",
       "      <td>0.590911</td>\n",
       "      <td>0.528095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m25p3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034218</td>\n",
       "      <td>0.749334</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>0.263940</td>\n",
       "      <td>0.329302</td>\n",
       "      <td>0.510820</td>\n",
       "      <td>0.732278</td>\n",
       "      <td>0.665082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>square</th>\n",
       "      <td>0.034218</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.034218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.573299</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.007378</td>\n",
       "      <td>0.015410</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.027619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp</th>\n",
       "      <td>0.749334</td>\n",
       "      <td>0.500323</td>\n",
       "      <td>0.749334</td>\n",
       "      <td>0.573299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.198140</td>\n",
       "      <td>0.239012</td>\n",
       "      <td>0.376332</td>\n",
       "      <td>0.545338</td>\n",
       "      <td>0.503078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp</th>\n",
       "      <td>0.005513</td>\n",
       "      <td>0.009729</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739261</td>\n",
       "      <td>0.670881</td>\n",
       "      <td>0.500491</td>\n",
       "      <td>0.275455</td>\n",
       "      <td>0.345686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1c3</th>\n",
       "      <td>0.263940</td>\n",
       "      <td>0.209910</td>\n",
       "      <td>0.263940</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.198140</td>\n",
       "      <td>0.739261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.581410</td>\n",
       "      <td>0.758619</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.427058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1c2</th>\n",
       "      <td>0.329302</td>\n",
       "      <td>0.271262</td>\n",
       "      <td>0.329302</td>\n",
       "      <td>0.007378</td>\n",
       "      <td>0.239012</td>\n",
       "      <td>0.670881</td>\n",
       "      <td>0.581410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.510932</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.005565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1c1</th>\n",
       "      <td>0.510820</td>\n",
       "      <td>0.406129</td>\n",
       "      <td>0.510820</td>\n",
       "      <td>0.015410</td>\n",
       "      <td>0.376332</td>\n",
       "      <td>0.500491</td>\n",
       "      <td>0.758619</td>\n",
       "      <td>0.510932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.251179</td>\n",
       "      <td>0.500261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b3c1</th>\n",
       "      <td>0.732278</td>\n",
       "      <td>0.590911</td>\n",
       "      <td>0.732278</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.545338</td>\n",
       "      <td>0.275455</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.251179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.580698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2c1</th>\n",
       "      <td>0.665082</td>\n",
       "      <td>0.528095</td>\n",
       "      <td>0.665082</td>\n",
       "      <td>0.027619</td>\n",
       "      <td>0.503078</td>\n",
       "      <td>0.345686</td>\n",
       "      <td>0.427058</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>0.500261</td>\n",
       "      <td>0.580698</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             base   base_01     m25p3    square       exp      comp      b1c3  \\\n",
       "base     1.000000  0.798438  1.000000  0.034218  0.749334  0.005513  0.263940   \n",
       "base_01  0.798438  1.000000  0.798438  0.002805  0.500323  0.009729  0.209910   \n",
       "m25p3    1.000000  0.798438  1.000000  0.034218  0.749334  0.005513  0.263940   \n",
       "square   0.034218  0.002805  0.034218  1.000000  0.573299  0.001166  0.007600   \n",
       "exp      0.749334  0.500323  0.749334  0.573299  1.000000  0.001150  0.198140   \n",
       "comp     0.005513  0.009729  0.005513  0.001166  0.001150  1.000000  0.739261   \n",
       "b1c3     0.263940  0.209910  0.263940  0.007600  0.198140  0.739261  1.000000   \n",
       "b1c2     0.329302  0.271262  0.329302  0.007378  0.239012  0.670881  0.581410   \n",
       "b1c1     0.510820  0.406129  0.510820  0.015410  0.376332  0.500491  0.758619   \n",
       "b3c1     0.732278  0.590911  0.732278  0.027486  0.545338  0.275455  0.005514   \n",
       "b2c1     0.665082  0.528095  0.665082  0.027619  0.503078  0.345686  0.427058   \n",
       "\n",
       "             b1c2      b1c1      b3c1      b2c1  \n",
       "base     0.329302  0.510820  0.732278  0.665082  \n",
       "base_01  0.271262  0.406129  0.590911  0.528095  \n",
       "m25p3    0.329302  0.510820  0.732278  0.665082  \n",
       "square   0.007378  0.015410  0.027486  0.027619  \n",
       "exp      0.239012  0.376332  0.545338  0.503078  \n",
       "comp     0.670881  0.500491  0.275455  0.345686  \n",
       "b1c3     0.581410  0.758619  0.005514  0.427058  \n",
       "b1c2     1.000000  0.510932  0.421900  0.005565  \n",
       "b1c1     0.510932  1.000000  0.251179  0.500261  \n",
       "b3c1     0.421900  0.251179  1.000000  0.580698  \n",
       "b2c1     0.005565  0.500261  0.580698  1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 特征之间的相关性\n",
    "random_1v4 = [1] * 2500 + [0] * 7500\n",
    "random.seed(211113)\n",
    "df_random_corr = pd.DataFrame()\n",
    "df_random_corr['base'] = np.random.normal(size=[10000])\n",
    "df_random_corr['base_01'] = df_random_corr['base'].map(lambda x: 0 if x < 0 else 1)  # 分类与回归的相关性\n",
    "df_random_corr['m25p3'] = df_random_corr['base'] * 25 + 3\n",
    "df_random_corr['square'] = df_random_corr['base'] ** 2\n",
    "df_random_corr['exp'] = np.exp(df_random_corr['base'])\n",
    "df_random_corr['comp'] = np.random.normal(size=[10000])\n",
    "\n",
    "df_random_corr['b1c3'] = df_random_corr['comp']\n",
    "df_random_corr['b1c3'][list(range(0, 10000, 4))] = df_random_corr['base'][list(range(0, 10000, 4))]\n",
    "\n",
    "df_random_corr['b1c2'] = df_random_corr['comp']\n",
    "df_random_corr['b1c2'][list(range(0, 10000, 3))] = df_random_corr['base'][list(range(0, 10000, 3))]\n",
    "\n",
    "df_random_corr['b1c1'] = df_random_corr['comp']\n",
    "df_random_corr['b1c1'][list(range(0, 10000, 2))] = df_random_corr['base'][list(range(0, 10000, 2))]\n",
    "\n",
    "df_random_corr['b3c1'] = df_random_corr['base']\n",
    "df_random_corr['b3c1'][list(range(0, 10000, 4))] = df_random_corr['comp'][list(range(0, 10000, 4))]\n",
    "\n",
    "df_random_corr['b2c1'] = df_random_corr['base']\n",
    "df_random_corr['b2c1'][list(range(0, 10000, 3))] = df_random_corr['comp'][list(range(0, 10000, 3))]\n",
    "\n",
    "df_random_corr.corr()\n",
    "# 结论，把正态转换成0-1，相关性大约80%\n",
    "# 和平方基本是不相关的：1、相关是线性相关，2、这种信息，需要非线性的模型才能提取出来。\n",
    "# exp非常有意思，和 square 和 base都有较高的相关性，进一步说明非线性的情况与线性的情况并不相同。\n",
    "# base 和 comp 的相关性与占比是成比例的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_0</th>\n",
       "      <th>n_1</th>\n",
       "      <th>n_2</th>\n",
       "      <th>n_3</th>\n",
       "      <th>n_4</th>\n",
       "      <th>n_5</th>\n",
       "      <th>n_6</th>\n",
       "      <th>n_7</th>\n",
       "      <th>n_8</th>\n",
       "      <th>n_9</th>\n",
       "      <th>sum</th>\n",
       "      <th>sum_01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>-0.002544</td>\n",
       "      <td>0.010814</td>\n",
       "      <td>0.003962</td>\n",
       "      <td>-0.004043</td>\n",
       "      <td>-0.010905</td>\n",
       "      <td>-0.010753</td>\n",
       "      <td>0.022728</td>\n",
       "      <td>0.323780</td>\n",
       "      <td>0.261753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_1</th>\n",
       "      <td>0.002026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014566</td>\n",
       "      <td>-0.006571</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>-0.000293</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>0.326560</td>\n",
       "      <td>0.264642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_2</th>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.014566</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011371</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>-0.001571</td>\n",
       "      <td>-0.012365</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.322864</td>\n",
       "      <td>0.253221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_3</th>\n",
       "      <td>-0.002544</td>\n",
       "      <td>-0.006571</td>\n",
       "      <td>0.011371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>-0.007792</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>-0.018698</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>0.007806</td>\n",
       "      <td>0.311387</td>\n",
       "      <td>0.259297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_4</th>\n",
       "      <td>0.010814</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004205</td>\n",
       "      <td>-0.011406</td>\n",
       "      <td>-0.003303</td>\n",
       "      <td>0.016112</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.325696</td>\n",
       "      <td>0.256049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_5</th>\n",
       "      <td>0.003962</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>-0.007792</td>\n",
       "      <td>-0.004205</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>-0.011617</td>\n",
       "      <td>-0.002537</td>\n",
       "      <td>0.307853</td>\n",
       "      <td>0.249099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_6</th>\n",
       "      <td>-0.004043</td>\n",
       "      <td>-0.000293</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>-0.011406</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001999</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.317831</td>\n",
       "      <td>0.253828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_7</th>\n",
       "      <td>-0.010905</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>-0.001571</td>\n",
       "      <td>-0.018698</td>\n",
       "      <td>-0.003303</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>-0.001999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013865</td>\n",
       "      <td>-0.002423</td>\n",
       "      <td>0.300584</td>\n",
       "      <td>0.233157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_8</th>\n",
       "      <td>-0.010753</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.012365</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>0.016112</td>\n",
       "      <td>-0.011617</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>-0.013865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017197</td>\n",
       "      <td>0.303799</td>\n",
       "      <td>0.239583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_9</th>\n",
       "      <td>0.022728</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.007806</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>-0.002537</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>-0.002423</td>\n",
       "      <td>-0.017197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.330971</td>\n",
       "      <td>0.268229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>0.323780</td>\n",
       "      <td>0.326560</td>\n",
       "      <td>0.322864</td>\n",
       "      <td>0.311387</td>\n",
       "      <td>0.325696</td>\n",
       "      <td>0.307853</td>\n",
       "      <td>0.317831</td>\n",
       "      <td>0.300584</td>\n",
       "      <td>0.303799</td>\n",
       "      <td>0.330971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_01</th>\n",
       "      <td>0.261753</td>\n",
       "      <td>0.264642</td>\n",
       "      <td>0.253221</td>\n",
       "      <td>0.259297</td>\n",
       "      <td>0.256049</td>\n",
       "      <td>0.249099</td>\n",
       "      <td>0.253828</td>\n",
       "      <td>0.233157</td>\n",
       "      <td>0.239583</td>\n",
       "      <td>0.268229</td>\n",
       "      <td>0.800524</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             n_0       n_1       n_2       n_3       n_4       n_5       n_6  \\\n",
       "n_0     1.000000  0.002026  0.003992 -0.002544  0.010814  0.003962 -0.004043   \n",
       "n_1     0.002026  1.000000  0.014566 -0.006571  0.006056  0.002158 -0.000293   \n",
       "n_2     0.003992  0.014566  1.000000  0.011371 -0.000534 -0.000387  0.003493   \n",
       "n_3    -0.002544 -0.006571  0.011371  1.000000  0.001239 -0.007792  0.002725   \n",
       "n_4     0.010814  0.006056 -0.000534  0.001239  1.000000 -0.004205 -0.011406   \n",
       "n_5     0.003962  0.002158 -0.000387 -0.007792 -0.004205  1.000000  0.000773   \n",
       "n_6    -0.004043 -0.000293  0.003493  0.002725 -0.011406  0.000773  1.000000   \n",
       "n_7    -0.010905  0.001066 -0.001571 -0.018698 -0.003303  0.002687 -0.001999   \n",
       "n_8    -0.010753 -0.000049 -0.012365  0.015758  0.016112 -0.011617  0.006734   \n",
       "n_9     0.022728  0.014501  0.006276  0.007806  0.013021 -0.002537  0.004154   \n",
       "sum     0.323780  0.326560  0.322864  0.311387  0.325696  0.307853  0.317831   \n",
       "sum_01  0.261753  0.264642  0.253221  0.259297  0.256049  0.249099  0.253828   \n",
       "\n",
       "             n_7       n_8       n_9       sum    sum_01  \n",
       "n_0    -0.010905 -0.010753  0.022728  0.323780  0.261753  \n",
       "n_1     0.001066 -0.000049  0.014501  0.326560  0.264642  \n",
       "n_2    -0.001571 -0.012365  0.006276  0.322864  0.253221  \n",
       "n_3    -0.018698  0.015758  0.007806  0.311387  0.259297  \n",
       "n_4    -0.003303  0.016112  0.013021  0.325696  0.256049  \n",
       "n_5     0.002687 -0.011617 -0.002537  0.307853  0.249099  \n",
       "n_6    -0.001999  0.006734  0.004154  0.317831  0.253828  \n",
       "n_7     1.000000 -0.013865 -0.002423  0.300584  0.233157  \n",
       "n_8    -0.013865  1.000000 -0.017197  0.303799  0.239583  \n",
       "n_9    -0.002423 -0.017197  1.000000  0.330971  0.268229  \n",
       "sum     0.300584  0.303799  0.330971  1.000000  0.800524  \n",
       "sum_01  0.233157  0.239583  0.268229  0.800524  1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 求和的情况\n",
    "df_random_corr = pd.DataFrame()\n",
    "random.seed(211113)\n",
    "for ii in range(10):\n",
    "    df_random_corr[f'n_{ii}'] = np.random.normal(size=[10000])\n",
    "\n",
    "df_random_corr['sum'] = df_random_corr.sum(axis=1)\n",
    "df_random_corr['sum_01'] = df_random_corr['sum'].map(lambda x: 0 if x < 0 else 1)  # 分类与回归的相关性\n",
    "df_random_corr.corr()\n",
    "# 我只是1/10，但相关系数到30%以上，即使到到0-2，也有1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_0</th>\n",
       "      <th>n_1</th>\n",
       "      <th>n_2</th>\n",
       "      <th>n_3</th>\n",
       "      <th>n_4</th>\n",
       "      <th>n_5</th>\n",
       "      <th>n_6</th>\n",
       "      <th>n_7</th>\n",
       "      <th>n_8</th>\n",
       "      <th>n_9</th>\n",
       "      <th>dot</th>\n",
       "      <th>dot_01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>-0.007891</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.011341</td>\n",
       "      <td>-0.011651</td>\n",
       "      <td>-0.000911</td>\n",
       "      <td>-0.017796</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>0.038896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_1</th>\n",
       "      <td>0.007285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002031</td>\n",
       "      <td>-0.016721</td>\n",
       "      <td>-0.001129</td>\n",
       "      <td>-0.002934</td>\n",
       "      <td>-0.000581</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>-0.013862</td>\n",
       "      <td>-0.007545</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>0.076464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_2</th>\n",
       "      <td>-0.000288</td>\n",
       "      <td>-0.002031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015255</td>\n",
       "      <td>-0.009200</td>\n",
       "      <td>-0.003412</td>\n",
       "      <td>-0.002079</td>\n",
       "      <td>-0.004962</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.115823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_3</th>\n",
       "      <td>-0.007891</td>\n",
       "      <td>-0.016721</td>\n",
       "      <td>0.015255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000560</td>\n",
       "      <td>-0.012051</td>\n",
       "      <td>-0.011112</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>0.016238</td>\n",
       "      <td>0.199612</td>\n",
       "      <td>0.161102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_4</th>\n",
       "      <td>0.002141</td>\n",
       "      <td>-0.001129</td>\n",
       "      <td>-0.009200</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002270</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>-0.008232</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.259341</td>\n",
       "      <td>0.208241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_5</th>\n",
       "      <td>0.000476</td>\n",
       "      <td>-0.002934</td>\n",
       "      <td>-0.003412</td>\n",
       "      <td>-0.000560</td>\n",
       "      <td>-0.002270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>-0.003851</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.305944</td>\n",
       "      <td>0.248282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_6</th>\n",
       "      <td>0.011341</td>\n",
       "      <td>-0.000581</td>\n",
       "      <td>-0.002079</td>\n",
       "      <td>-0.012051</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>-0.004721</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.352748</td>\n",
       "      <td>0.282178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_7</th>\n",
       "      <td>-0.011651</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>-0.004962</td>\n",
       "      <td>-0.011112</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.419450</td>\n",
       "      <td>0.329372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_8</th>\n",
       "      <td>-0.000911</td>\n",
       "      <td>-0.013862</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>-0.008232</td>\n",
       "      <td>-0.003851</td>\n",
       "      <td>-0.004721</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015325</td>\n",
       "      <td>0.439826</td>\n",
       "      <td>0.349104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_9</th>\n",
       "      <td>-0.017796</td>\n",
       "      <td>-0.007545</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>0.016238</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>-0.015325</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.512330</td>\n",
       "      <td>0.410435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dot</th>\n",
       "      <td>0.040613</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.199612</td>\n",
       "      <td>0.259341</td>\n",
       "      <td>0.305944</td>\n",
       "      <td>0.352748</td>\n",
       "      <td>0.419450</td>\n",
       "      <td>0.439826</td>\n",
       "      <td>0.512330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.797910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dot_01</th>\n",
       "      <td>0.038896</td>\n",
       "      <td>0.076464</td>\n",
       "      <td>0.115823</td>\n",
       "      <td>0.161102</td>\n",
       "      <td>0.208241</td>\n",
       "      <td>0.248282</td>\n",
       "      <td>0.282178</td>\n",
       "      <td>0.329372</td>\n",
       "      <td>0.349104</td>\n",
       "      <td>0.410435</td>\n",
       "      <td>0.797910</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             n_0       n_1       n_2       n_3       n_4       n_5       n_6  \\\n",
       "n_0     1.000000  0.007285 -0.000288 -0.007891  0.002141  0.000476  0.011341   \n",
       "n_1     0.007285  1.000000 -0.002031 -0.016721 -0.001129 -0.002934 -0.000581   \n",
       "n_2    -0.000288 -0.002031  1.000000  0.015255 -0.009200 -0.003412 -0.002079   \n",
       "n_3    -0.007891 -0.016721  0.015255  1.000000 -0.000003 -0.000560 -0.012051   \n",
       "n_4     0.002141 -0.001129 -0.009200 -0.000003  1.000000 -0.002270  0.002991   \n",
       "n_5     0.000476 -0.002934 -0.003412 -0.000560 -0.002270  1.000000  0.001802   \n",
       "n_6     0.011341 -0.000581 -0.002079 -0.012051  0.002991  0.001802  1.000000   \n",
       "n_7    -0.011651  0.011669 -0.004962 -0.011112  0.006772  0.001485  0.000279   \n",
       "n_8    -0.000911 -0.013862  0.003163 -0.012643 -0.008232 -0.003851 -0.004721   \n",
       "n_9    -0.017796 -0.007545 -0.001746  0.016238  0.003007  0.004378  0.001561   \n",
       "dot     0.040613  0.091791  0.150865  0.199612  0.259341  0.305944  0.352748   \n",
       "dot_01  0.038896  0.076464  0.115823  0.161102  0.208241  0.248282  0.282178   \n",
       "\n",
       "             n_7       n_8       n_9       dot    dot_01  \n",
       "n_0    -0.011651 -0.000911 -0.017796  0.040613  0.038896  \n",
       "n_1     0.011669 -0.013862 -0.007545  0.091791  0.076464  \n",
       "n_2    -0.004962  0.003163 -0.001746  0.150865  0.115823  \n",
       "n_3    -0.011112 -0.012643  0.016238  0.199612  0.161102  \n",
       "n_4     0.006772 -0.008232  0.003007  0.259341  0.208241  \n",
       "n_5     0.001485 -0.003851  0.004378  0.305944  0.248282  \n",
       "n_6     0.000279 -0.004721  0.001561  0.352748  0.282178  \n",
       "n_7     1.000000  0.002369  0.003650  0.419450  0.329372  \n",
       "n_8     0.002369  1.000000 -0.015325  0.439826  0.349104  \n",
       "n_9     0.003650 -0.015325  1.000000  0.512330  0.410435  \n",
       "dot     0.419450  0.439826  0.512330  1.000000  0.797910  \n",
       "dot_01  0.329372  0.349104  0.410435  0.797910  1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 线性组合的情况\n",
    "# dot = [n_0, n_1, n_2, n_3, n_4, n_5, n_6, n_7, n_8, n_9] 内积 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "df_random_corr = pd.DataFrame()\n",
    "random.seed(211113)\n",
    "for ii in range(10):\n",
    "    df_random_corr[f'n_{ii}'] = np.random.normal(size=[10000])\n",
    "\n",
    "df_random_corr['dot'] = df_random_corr.apply(lambda x: np.dot(x, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), axis=1)\n",
    "df_random_corr['dot_01'] = df_random_corr['dot'].map(lambda x: 0 if x < 0 else 1)  # 分类与回归的相关性\n",
    "df_random_corr.corr()\n",
    "# 大力确实可以出奇迹\n",
    "# 这也是数据标准化的依据（之一）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_0</th>\n",
       "      <th>n_1</th>\n",
       "      <th>n_2</th>\n",
       "      <th>n_3</th>\n",
       "      <th>n_4</th>\n",
       "      <th>n_5</th>\n",
       "      <th>n_6</th>\n",
       "      <th>n_7</th>\n",
       "      <th>n_8</th>\n",
       "      <th>n_9</th>\n",
       "      <th>rule</th>\n",
       "      <th>rule_01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>-0.003591</td>\n",
       "      <td>0.008119</td>\n",
       "      <td>-0.011746</td>\n",
       "      <td>-0.001143</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.017321</td>\n",
       "      <td>0.073430</td>\n",
       "      <td>0.065313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_1</th>\n",
       "      <td>0.010909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003274</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>0.008734</td>\n",
       "      <td>-0.005652</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.007226</td>\n",
       "      <td>0.018165</td>\n",
       "      <td>0.027505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_2</th>\n",
       "      <td>0.000726</td>\n",
       "      <td>-0.003274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018714</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>0.007541</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0.015731</td>\n",
       "      <td>-0.003539</td>\n",
       "      <td>0.255035</td>\n",
       "      <td>0.212090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_3</th>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>-0.018714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>-0.003533</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.010647</td>\n",
       "      <td>-0.012470</td>\n",
       "      <td>-0.013892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_4</th>\n",
       "      <td>-0.003591</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019016</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.003412</td>\n",
       "      <td>-0.010621</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.388197</td>\n",
       "      <td>0.296633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_5</th>\n",
       "      <td>0.008119</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>-0.003533</td>\n",
       "      <td>0.019016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017597</td>\n",
       "      <td>0.007995</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>-0.008506</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.002983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_6</th>\n",
       "      <td>-0.011746</td>\n",
       "      <td>0.008734</td>\n",
       "      <td>0.007541</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>-0.017597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>-0.005139</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>0.540471</td>\n",
       "      <td>0.430296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_7</th>\n",
       "      <td>-0.001143</td>\n",
       "      <td>-0.005652</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.003412</td>\n",
       "      <td>0.007995</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>-0.003124</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.013166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_8</th>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.015731</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.010621</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>-0.005139</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.696757</td>\n",
       "      <td>0.558217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_9</th>\n",
       "      <td>0.017321</td>\n",
       "      <td>0.007226</td>\n",
       "      <td>-0.003539</td>\n",
       "      <td>-0.010647</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-0.008506</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>-0.003124</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006621</td>\n",
       "      <td>0.007474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rule</th>\n",
       "      <td>0.073430</td>\n",
       "      <td>0.018165</td>\n",
       "      <td>0.255035</td>\n",
       "      <td>-0.012470</td>\n",
       "      <td>0.388197</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.540471</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.696757</td>\n",
       "      <td>0.006621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rule_01</th>\n",
       "      <td>0.065313</td>\n",
       "      <td>0.027505</td>\n",
       "      <td>0.212090</td>\n",
       "      <td>-0.013892</td>\n",
       "      <td>0.296633</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.430296</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>0.558217</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>0.796385</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              n_0       n_1       n_2       n_3       n_4       n_5       n_6  \\\n",
       "n_0      1.000000  0.010909  0.000726  0.000709 -0.003591  0.008119 -0.011746   \n",
       "n_1      0.010909  1.000000 -0.003274  0.004502  0.013316 -0.010926  0.008734   \n",
       "n_2      0.000726 -0.003274  1.000000 -0.018714  0.004373 -0.001969  0.007541   \n",
       "n_3      0.000709  0.004502 -0.018714  1.000000  0.004952 -0.003533 -0.018317   \n",
       "n_4     -0.003591  0.013316  0.004373  0.004952  1.000000  0.019016  0.001785   \n",
       "n_5      0.008119 -0.010926 -0.001969 -0.003533  0.019016  1.000000 -0.017597   \n",
       "n_6     -0.011746  0.008734  0.007541 -0.018317  0.001785 -0.017597  1.000000   \n",
       "n_7     -0.001143 -0.005652  0.006745  0.007030  0.003412  0.007995  0.009926   \n",
       "n_8      0.004846  0.011599  0.015731 -0.000117 -0.010621  0.007573 -0.005139   \n",
       "n_9      0.017321  0.007226 -0.003539 -0.010647  0.001300 -0.008506  0.009918   \n",
       "rule     0.073430  0.018165  0.255035 -0.012470  0.388197  0.003404  0.540471   \n",
       "rule_01  0.065313  0.027505  0.212090 -0.013892  0.296633  0.002983  0.430296   \n",
       "\n",
       "              n_7       n_8       n_9      rule   rule_01  \n",
       "n_0     -0.001143  0.004846  0.017321  0.073430  0.065313  \n",
       "n_1     -0.005652  0.011599  0.007226  0.018165  0.027505  \n",
       "n_2      0.006745  0.015731 -0.003539  0.255035  0.212090  \n",
       "n_3      0.007030 -0.000117 -0.010647 -0.012470 -0.013892  \n",
       "n_4      0.003412 -0.010621  0.001300  0.388197  0.296633  \n",
       "n_5      0.007995  0.007573 -0.008506  0.003404  0.002983  \n",
       "n_6      0.009926 -0.005139  0.009918  0.540471  0.430296  \n",
       "n_7      1.000000  0.007521 -0.003124  0.013508  0.013166  \n",
       "n_8      0.007521  1.000000  0.000323  0.696757  0.558217  \n",
       "n_9     -0.003124  0.000323  1.000000  0.006621  0.007474  \n",
       "rule     0.013508  0.696757  0.006621  1.000000  0.796385  \n",
       "rule_01  0.013166  0.558217  0.007474  0.796385  1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 基于线性规则的模拟\n",
    "# dot = [n_0, n_1, n_2, n_3, n_4, n_5, n_6, n_7, n_8, n_9] 内积 [1, 0, 3, 0, 5, 0, 7, 0, 9, 0]\n",
    "df_random_corr = pd.DataFrame()\n",
    "random.seed(211113)\n",
    "for ii in range(10):\n",
    "    df_random_corr[f'n_{ii}'] = np.random.normal(size=[10000])\n",
    "\n",
    "df_random_corr['rule'] = df_random_corr.apply(lambda x: np.dot(x, [1, 0, 3, 0, 5, 0, 7, 0, 9, 0]), axis=1)\n",
    "df_random_corr['rule_01'] = df_random_corr['rule'].map(lambda x: 0 if x < 0 else 1)  # 分类与回归的相关性\n",
    "df_random_corr.corr()\n",
    "# 这个其实并没有什么意外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lgb 对内积情况下的 回归与分类的模拟\n",
    "# dot = [n_0, n_1, n_2, n_3, n_4, n_5, n_6, n_7, n_8, n_9] 内积 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "df_random_corr = pd.DataFrame()\n",
    "random.seed(211113)\n",
    "for ii in range(10):\n",
    "    df_random_corr[f'n_{ii}'] = np.random.normal(size=[10000 + 15015])\n",
    "\n",
    "df_random_corr['rule'] = df_random_corr.apply(lambda x: np.dot(x, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), axis=1)\n",
    "df_random_corr['rule_01'] = df_random_corr['rule'].map(lambda x: 0 if x < 0 else 1)  # 分类与回归的相关性\n",
    "df_random_corr.corr()\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = df_random_corr[['n_0', 'n_1', 'n_2', 'n_3', 'n_4', 'n_5', 'n_6', 'n_7', 'n_8', 'n_9']]\n",
    "train_value = df_random_corr['rule']\n",
    "train_class = df_random_corr['rule_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 7537, number of negative: 7478\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 15015, number of used features: 10\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501965 -> initscore=0.007859\n",
      "[LightGBM] [Info] Start training from score 0.007859\n",
      "[1]\tvalid_0's auc: 0.865154\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's auc: 0.887882\n",
      "[3]\tvalid_0's auc: 0.891855\n",
      "[4]\tvalid_0's auc: 0.91012\n",
      "[5]\tvalid_0's auc: 0.926074\n",
      "[6]\tvalid_0's auc: 0.925852\n",
      "[7]\tvalid_0's auc: 0.932099\n",
      "[8]\tvalid_0's auc: 0.936378\n",
      "[9]\tvalid_0's auc: 0.944391\n",
      "[10]\tvalid_0's auc: 0.9469\n",
      "[11]\tvalid_0's auc: 0.950452\n",
      "[12]\tvalid_0's auc: 0.955378\n",
      "[13]\tvalid_0's auc: 0.957148\n",
      "[14]\tvalid_0's auc: 0.958615\n",
      "[15]\tvalid_0's auc: 0.959577\n",
      "[16]\tvalid_0's auc: 0.960048\n",
      "[17]\tvalid_0's auc: 0.961197\n",
      "[18]\tvalid_0's auc: 0.962971\n",
      "[19]\tvalid_0's auc: 0.963287\n",
      "[20]\tvalid_0's auc: 0.965063\n",
      "[21]\tvalid_0's auc: 0.96588\n",
      "[22]\tvalid_0's auc: 0.967858\n",
      "[23]\tvalid_0's auc: 0.968692\n",
      "[24]\tvalid_0's auc: 0.9701\n",
      "[25]\tvalid_0's auc: 0.971094\n",
      "[26]\tvalid_0's auc: 0.972427\n",
      "[27]\tvalid_0's auc: 0.974099\n",
      "[28]\tvalid_0's auc: 0.975638\n",
      "[29]\tvalid_0's auc: 0.976129\n",
      "[30]\tvalid_0's auc: 0.976995\n",
      "[31]\tvalid_0's auc: 0.977382\n",
      "[32]\tvalid_0's auc: 0.977945\n",
      "[33]\tvalid_0's auc: 0.978726\n",
      "[34]\tvalid_0's auc: 0.979006\n",
      "[35]\tvalid_0's auc: 0.979645\n",
      "[36]\tvalid_0's auc: 0.97998\n",
      "[37]\tvalid_0's auc: 0.980361\n",
      "[38]\tvalid_0's auc: 0.981232\n",
      "[39]\tvalid_0's auc: 0.982089\n",
      "[40]\tvalid_0's auc: 0.982739\n",
      "[41]\tvalid_0's auc: 0.982671\n",
      "[42]\tvalid_0's auc: 0.983345\n",
      "[43]\tvalid_0's auc: 0.983436\n",
      "[44]\tvalid_0's auc: 0.983933\n",
      "[45]\tvalid_0's auc: 0.984062\n",
      "[46]\tvalid_0's auc: 0.984367\n",
      "[47]\tvalid_0's auc: 0.984938\n",
      "[48]\tvalid_0's auc: 0.985464\n",
      "[49]\tvalid_0's auc: 0.985638\n",
      "[50]\tvalid_0's auc: 0.98562\n",
      "[51]\tvalid_0's auc: 0.985974\n",
      "[52]\tvalid_0's auc: 0.986158\n",
      "[53]\tvalid_0's auc: 0.98661\n",
      "[54]\tvalid_0's auc: 0.986866\n",
      "[55]\tvalid_0's auc: 0.987177\n",
      "[56]\tvalid_0's auc: 0.98767\n",
      "[57]\tvalid_0's auc: 0.988105\n",
      "[58]\tvalid_0's auc: 0.988356\n",
      "[59]\tvalid_0's auc: 0.988674\n",
      "[60]\tvalid_0's auc: 0.988732\n",
      "[61]\tvalid_0's auc: 0.988943\n",
      "[62]\tvalid_0's auc: 0.988854\n",
      "[63]\tvalid_0's auc: 0.989011\n",
      "[64]\tvalid_0's auc: 0.989373\n",
      "[65]\tvalid_0's auc: 0.989845\n",
      "[66]\tvalid_0's auc: 0.99018\n",
      "[67]\tvalid_0's auc: 0.990471\n",
      "[68]\tvalid_0's auc: 0.990668\n",
      "[69]\tvalid_0's auc: 0.990545\n",
      "[70]\tvalid_0's auc: 0.990699\n",
      "[71]\tvalid_0's auc: 0.990884\n",
      "[72]\tvalid_0's auc: 0.991058\n",
      "[73]\tvalid_0's auc: 0.991157\n",
      "[74]\tvalid_0's auc: 0.991064\n",
      "[75]\tvalid_0's auc: 0.991124\n",
      "[76]\tvalid_0's auc: 0.991148\n",
      "[77]\tvalid_0's auc: 0.991122\n",
      "[78]\tvalid_0's auc: 0.991123\n",
      "[79]\tvalid_0's auc: 0.991069\n",
      "[80]\tvalid_0's auc: 0.991215\n",
      "[81]\tvalid_0's auc: 0.991325\n",
      "[82]\tvalid_0's auc: 0.991552\n",
      "[83]\tvalid_0's auc: 0.991647\n",
      "[84]\tvalid_0's auc: 0.991646\n",
      "[85]\tvalid_0's auc: 0.991884\n",
      "[86]\tvalid_0's auc: 0.991805\n",
      "[87]\tvalid_0's auc: 0.991863\n",
      "[88]\tvalid_0's auc: 0.991961\n",
      "[89]\tvalid_0's auc: 0.992089\n",
      "[90]\tvalid_0's auc: 0.99212\n",
      "[91]\tvalid_0's auc: 0.99222\n",
      "[92]\tvalid_0's auc: 0.992373\n",
      "[93]\tvalid_0's auc: 0.992294\n",
      "[94]\tvalid_0's auc: 0.992368\n",
      "[95]\tvalid_0's auc: 0.99249\n",
      "[96]\tvalid_0's auc: 0.992516\n",
      "[97]\tvalid_0's auc: 0.992543\n",
      "[98]\tvalid_0's auc: 0.99254\n",
      "[99]\tvalid_0's auc: 0.992595\n",
      "[100]\tvalid_0's auc: 0.992647\n",
      "[101]\tvalid_0's auc: 0.99268\n",
      "[102]\tvalid_0's auc: 0.992741\n",
      "[103]\tvalid_0's auc: 0.992873\n",
      "[104]\tvalid_0's auc: 0.992901\n",
      "[105]\tvalid_0's auc: 0.99292\n",
      "[106]\tvalid_0's auc: 0.992972\n",
      "[107]\tvalid_0's auc: 0.993053\n",
      "[108]\tvalid_0's auc: 0.993128\n",
      "[109]\tvalid_0's auc: 0.993203\n",
      "[110]\tvalid_0's auc: 0.993185\n",
      "[111]\tvalid_0's auc: 0.993281\n",
      "[112]\tvalid_0's auc: 0.993389\n",
      "[113]\tvalid_0's auc: 0.993405\n",
      "[114]\tvalid_0's auc: 0.993426\n",
      "[115]\tvalid_0's auc: 0.993429\n",
      "[116]\tvalid_0's auc: 0.993514\n",
      "[117]\tvalid_0's auc: 0.993557\n",
      "[118]\tvalid_0's auc: 0.99361\n",
      "[119]\tvalid_0's auc: 0.993715\n",
      "[120]\tvalid_0's auc: 0.993757\n",
      "[121]\tvalid_0's auc: 0.993776\n",
      "[122]\tvalid_0's auc: 0.993782\n",
      "[123]\tvalid_0's auc: 0.993872\n",
      "[124]\tvalid_0's auc: 0.993919\n",
      "[125]\tvalid_0's auc: 0.993935\n",
      "[126]\tvalid_0's auc: 0.993914\n",
      "[127]\tvalid_0's auc: 0.993942\n",
      "[128]\tvalid_0's auc: 0.993951\n",
      "[129]\tvalid_0's auc: 0.993997\n",
      "[130]\tvalid_0's auc: 0.99402\n",
      "[131]\tvalid_0's auc: 0.993989\n",
      "[132]\tvalid_0's auc: 0.99403\n",
      "[133]\tvalid_0's auc: 0.994067\n",
      "[134]\tvalid_0's auc: 0.994128\n",
      "[135]\tvalid_0's auc: 0.994095\n",
      "[136]\tvalid_0's auc: 0.994137\n",
      "[137]\tvalid_0's auc: 0.994163\n",
      "[138]\tvalid_0's auc: 0.994214\n",
      "[139]\tvalid_0's auc: 0.994251\n",
      "[140]\tvalid_0's auc: 0.994284\n",
      "[141]\tvalid_0's auc: 0.994264\n",
      "[142]\tvalid_0's auc: 0.994319\n",
      "[143]\tvalid_0's auc: 0.994349\n",
      "[144]\tvalid_0's auc: 0.994368\n",
      "[145]\tvalid_0's auc: 0.994361\n",
      "[146]\tvalid_0's auc: 0.994382\n",
      "[147]\tvalid_0's auc: 0.994415\n",
      "[148]\tvalid_0's auc: 0.994396\n",
      "[149]\tvalid_0's auc: 0.994444\n",
      "[150]\tvalid_0's auc: 0.994487\n",
      "[151]\tvalid_0's auc: 0.994507\n",
      "[152]\tvalid_0's auc: 0.994492\n",
      "[153]\tvalid_0's auc: 0.994471\n",
      "[154]\tvalid_0's auc: 0.994509\n",
      "[155]\tvalid_0's auc: 0.994506\n",
      "[156]\tvalid_0's auc: 0.994549\n",
      "[157]\tvalid_0's auc: 0.994529\n",
      "[158]\tvalid_0's auc: 0.994595\n",
      "[159]\tvalid_0's auc: 0.994646\n",
      "[160]\tvalid_0's auc: 0.994698\n",
      "[161]\tvalid_0's auc: 0.994706\n",
      "[162]\tvalid_0's auc: 0.994716\n",
      "[163]\tvalid_0's auc: 0.99473\n",
      "[164]\tvalid_0's auc: 0.994665\n",
      "[165]\tvalid_0's auc: 0.994719\n",
      "[166]\tvalid_0's auc: 0.994727\n",
      "[167]\tvalid_0's auc: 0.994757\n",
      "[168]\tvalid_0's auc: 0.994828\n",
      "[169]\tvalid_0's auc: 0.994813\n",
      "[170]\tvalid_0's auc: 0.994806\n",
      "[171]\tvalid_0's auc: 0.994844\n",
      "[172]\tvalid_0's auc: 0.994831\n",
      "[173]\tvalid_0's auc: 0.99489\n",
      "[174]\tvalid_0's auc: 0.994916\n",
      "[175]\tvalid_0's auc: 0.994921\n",
      "[176]\tvalid_0's auc: 0.994972\n",
      "[177]\tvalid_0's auc: 0.994976\n",
      "[178]\tvalid_0's auc: 0.995031\n",
      "[179]\tvalid_0's auc: 0.995041\n",
      "[180]\tvalid_0's auc: 0.995033\n",
      "[181]\tvalid_0's auc: 0.995052\n",
      "[182]\tvalid_0's auc: 0.99507\n",
      "[183]\tvalid_0's auc: 0.995086\n",
      "[184]\tvalid_0's auc: 0.9951\n",
      "[185]\tvalid_0's auc: 0.995143\n",
      "[186]\tvalid_0's auc: 0.995172\n",
      "[187]\tvalid_0's auc: 0.995173\n",
      "[188]\tvalid_0's auc: 0.995166\n",
      "[189]\tvalid_0's auc: 0.995134\n",
      "[190]\tvalid_0's auc: 0.995149\n",
      "[191]\tvalid_0's auc: 0.995193\n",
      "[192]\tvalid_0's auc: 0.995188\n",
      "[193]\tvalid_0's auc: 0.995216\n",
      "[194]\tvalid_0's auc: 0.995266\n",
      "[195]\tvalid_0's auc: 0.995298\n",
      "[196]\tvalid_0's auc: 0.995296\n",
      "[197]\tvalid_0's auc: 0.995288\n",
      "[198]\tvalid_0's auc: 0.995304\n",
      "[199]\tvalid_0's auc: 0.995341\n",
      "[200]\tvalid_0's auc: 0.995346\n",
      "[201]\tvalid_0's auc: 0.995334\n",
      "[202]\tvalid_0's auc: 0.995344\n",
      "[203]\tvalid_0's auc: 0.995323\n",
      "[204]\tvalid_0's auc: 0.995331\n",
      "[205]\tvalid_0's auc: 0.995344\n",
      "[206]\tvalid_0's auc: 0.995359\n",
      "[207]\tvalid_0's auc: 0.995359\n",
      "[208]\tvalid_0's auc: 0.995393\n",
      "[209]\tvalid_0's auc: 0.995379\n",
      "[210]\tvalid_0's auc: 0.995369\n",
      "[211]\tvalid_0's auc: 0.995383\n",
      "[212]\tvalid_0's auc: 0.995364\n",
      "[213]\tvalid_0's auc: 0.995356\n",
      "[214]\tvalid_0's auc: 0.995377\n",
      "[215]\tvalid_0's auc: 0.995402\n",
      "[216]\tvalid_0's auc: 0.99541\n",
      "[217]\tvalid_0's auc: 0.995394\n",
      "[218]\tvalid_0's auc: 0.995398\n",
      "[219]\tvalid_0's auc: 0.9954\n",
      "[220]\tvalid_0's auc: 0.995414\n",
      "[221]\tvalid_0's auc: 0.995435\n",
      "[222]\tvalid_0's auc: 0.995407\n",
      "[223]\tvalid_0's auc: 0.995395\n",
      "[224]\tvalid_0's auc: 0.99542\n",
      "[225]\tvalid_0's auc: 0.995417\n",
      "[226]\tvalid_0's auc: 0.995427\n",
      "[227]\tvalid_0's auc: 0.995428\n",
      "[228]\tvalid_0's auc: 0.995414\n",
      "[229]\tvalid_0's auc: 0.995396\n",
      "[230]\tvalid_0's auc: 0.99541\n",
      "[231]\tvalid_0's auc: 0.995431\n",
      "[232]\tvalid_0's auc: 0.995417\n",
      "[233]\tvalid_0's auc: 0.995451\n",
      "[234]\tvalid_0's auc: 0.995446\n",
      "[235]\tvalid_0's auc: 0.995441\n",
      "[236]\tvalid_0's auc: 0.99544\n",
      "[237]\tvalid_0's auc: 0.99542\n",
      "[238]\tvalid_0's auc: 0.99543\n",
      "[239]\tvalid_0's auc: 0.995444\n",
      "[240]\tvalid_0's auc: 0.995443\n",
      "[241]\tvalid_0's auc: 0.995428\n",
      "[242]\tvalid_0's auc: 0.99543\n",
      "[243]\tvalid_0's auc: 0.995418\n",
      "[244]\tvalid_0's auc: 0.99542\n",
      "[245]\tvalid_0's auc: 0.995435\n",
      "[246]\tvalid_0's auc: 0.995442\n",
      "[247]\tvalid_0's auc: 0.995455\n",
      "[248]\tvalid_0's auc: 0.995444\n",
      "[249]\tvalid_0's auc: 0.995464\n",
      "[250]\tvalid_0's auc: 0.995461\n",
      "[251]\tvalid_0's auc: 0.995477\n",
      "[252]\tvalid_0's auc: 0.995498\n",
      "[253]\tvalid_0's auc: 0.995501\n",
      "[254]\tvalid_0's auc: 0.99549\n",
      "[255]\tvalid_0's auc: 0.995501\n",
      "[256]\tvalid_0's auc: 0.995514\n",
      "[257]\tvalid_0's auc: 0.995525\n",
      "[258]\tvalid_0's auc: 0.995527\n",
      "[259]\tvalid_0's auc: 0.995519\n",
      "[260]\tvalid_0's auc: 0.995527\n",
      "[261]\tvalid_0's auc: 0.995537\n",
      "[262]\tvalid_0's auc: 0.995529\n",
      "[263]\tvalid_0's auc: 0.995534\n",
      "[264]\tvalid_0's auc: 0.995534\n",
      "[265]\tvalid_0's auc: 0.995556\n",
      "[266]\tvalid_0's auc: 0.995567\n",
      "[267]\tvalid_0's auc: 0.995552\n",
      "[268]\tvalid_0's auc: 0.995573\n",
      "[269]\tvalid_0's auc: 0.995586\n",
      "[270]\tvalid_0's auc: 0.995604\n",
      "[271]\tvalid_0's auc: 0.995616\n",
      "[272]\tvalid_0's auc: 0.99561\n",
      "[273]\tvalid_0's auc: 0.99559\n",
      "[274]\tvalid_0's auc: 0.995593\n",
      "[275]\tvalid_0's auc: 0.995612\n",
      "[276]\tvalid_0's auc: 0.995603\n",
      "[277]\tvalid_0's auc: 0.995611\n",
      "[278]\tvalid_0's auc: 0.995621\n",
      "[279]\tvalid_0's auc: 0.995632\n",
      "[280]\tvalid_0's auc: 0.995639\n",
      "[281]\tvalid_0's auc: 0.99563\n",
      "[282]\tvalid_0's auc: 0.995632\n",
      "[283]\tvalid_0's auc: 0.995649\n",
      "[284]\tvalid_0's auc: 0.995669\n",
      "[285]\tvalid_0's auc: 0.995667\n",
      "[286]\tvalid_0's auc: 0.995666\n",
      "[287]\tvalid_0's auc: 0.995683\n",
      "[288]\tvalid_0's auc: 0.995687\n",
      "[289]\tvalid_0's auc: 0.995675\n",
      "[290]\tvalid_0's auc: 0.99566\n",
      "[291]\tvalid_0's auc: 0.995669\n",
      "[292]\tvalid_0's auc: 0.995684\n",
      "[293]\tvalid_0's auc: 0.995685\n",
      "[294]\tvalid_0's auc: 0.995707\n",
      "[295]\tvalid_0's auc: 0.995735\n",
      "[296]\tvalid_0's auc: 0.99573\n",
      "[297]\tvalid_0's auc: 0.995735\n",
      "[298]\tvalid_0's auc: 0.99574\n",
      "[299]\tvalid_0's auc: 0.995741\n",
      "[300]\tvalid_0's auc: 0.995745\n",
      "[301]\tvalid_0's auc: 0.99574\n",
      "[302]\tvalid_0's auc: 0.995756\n",
      "[303]\tvalid_0's auc: 0.995768\n",
      "[304]\tvalid_0's auc: 0.995753\n",
      "[305]\tvalid_0's auc: 0.99576\n",
      "[306]\tvalid_0's auc: 0.995776\n",
      "[307]\tvalid_0's auc: 0.995751\n",
      "[308]\tvalid_0's auc: 0.995751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[309]\tvalid_0's auc: 0.995732\n",
      "[310]\tvalid_0's auc: 0.995746\n",
      "[311]\tvalid_0's auc: 0.99576\n",
      "[312]\tvalid_0's auc: 0.995769\n",
      "[313]\tvalid_0's auc: 0.99579\n",
      "[314]\tvalid_0's auc: 0.995787\n",
      "[315]\tvalid_0's auc: 0.995788\n",
      "[316]\tvalid_0's auc: 0.995808\n",
      "[317]\tvalid_0's auc: 0.995796\n",
      "[318]\tvalid_0's auc: 0.995801\n",
      "[319]\tvalid_0's auc: 0.995803\n",
      "[320]\tvalid_0's auc: 0.995802\n",
      "[321]\tvalid_0's auc: 0.995802\n",
      "[322]\tvalid_0's auc: 0.9958\n",
      "[323]\tvalid_0's auc: 0.995806\n",
      "[324]\tvalid_0's auc: 0.995813\n",
      "[325]\tvalid_0's auc: 0.995815\n",
      "[326]\tvalid_0's auc: 0.995825\n",
      "[327]\tvalid_0's auc: 0.995837\n",
      "[328]\tvalid_0's auc: 0.995839\n",
      "[329]\tvalid_0's auc: 0.995831\n",
      "[330]\tvalid_0's auc: 0.995826\n",
      "[331]\tvalid_0's auc: 0.995828\n",
      "[332]\tvalid_0's auc: 0.995831\n",
      "[333]\tvalid_0's auc: 0.995846\n",
      "[334]\tvalid_0's auc: 0.995852\n",
      "[335]\tvalid_0's auc: 0.995853\n",
      "[336]\tvalid_0's auc: 0.995852\n",
      "[337]\tvalid_0's auc: 0.995863\n",
      "[338]\tvalid_0's auc: 0.995872\n",
      "[339]\tvalid_0's auc: 0.995866\n",
      "[340]\tvalid_0's auc: 0.995864\n",
      "[341]\tvalid_0's auc: 0.995881\n",
      "[342]\tvalid_0's auc: 0.99587\n",
      "[343]\tvalid_0's auc: 0.99586\n",
      "[344]\tvalid_0's auc: 0.995847\n",
      "[345]\tvalid_0's auc: 0.995839\n",
      "[346]\tvalid_0's auc: 0.99585\n",
      "[347]\tvalid_0's auc: 0.995869\n",
      "[348]\tvalid_0's auc: 0.99586\n",
      "[349]\tvalid_0's auc: 0.995863\n",
      "[350]\tvalid_0's auc: 0.99586\n",
      "[351]\tvalid_0's auc: 0.995855\n",
      "[352]\tvalid_0's auc: 0.995859\n",
      "[353]\tvalid_0's auc: 0.995852\n",
      "[354]\tvalid_0's auc: 0.995852\n",
      "[355]\tvalid_0's auc: 0.995845\n",
      "[356]\tvalid_0's auc: 0.99584\n",
      "[357]\tvalid_0's auc: 0.995843\n",
      "[358]\tvalid_0's auc: 0.995849\n",
      "[359]\tvalid_0's auc: 0.99585\n",
      "[360]\tvalid_0's auc: 0.995843\n",
      "[361]\tvalid_0's auc: 0.995842\n",
      "[362]\tvalid_0's auc: 0.995828\n",
      "[363]\tvalid_0's auc: 0.99584\n",
      "[364]\tvalid_0's auc: 0.995832\n",
      "[365]\tvalid_0's auc: 0.99584\n",
      "[366]\tvalid_0's auc: 0.995838\n",
      "[367]\tvalid_0's auc: 0.99582\n",
      "[368]\tvalid_0's auc: 0.99583\n",
      "[369]\tvalid_0's auc: 0.995842\n",
      "[370]\tvalid_0's auc: 0.995857\n",
      "[371]\tvalid_0's auc: 0.995863\n",
      "[372]\tvalid_0's auc: 0.995861\n",
      "[373]\tvalid_0's auc: 0.99585\n",
      "[374]\tvalid_0's auc: 0.995849\n",
      "[375]\tvalid_0's auc: 0.995843\n",
      "[376]\tvalid_0's auc: 0.995857\n",
      "[377]\tvalid_0's auc: 0.995857\n",
      "[378]\tvalid_0's auc: 0.995867\n",
      "[379]\tvalid_0's auc: 0.995861\n",
      "[380]\tvalid_0's auc: 0.995863\n",
      "[381]\tvalid_0's auc: 0.995875\n",
      "[382]\tvalid_0's auc: 0.995883\n",
      "[383]\tvalid_0's auc: 0.995888\n",
      "[384]\tvalid_0's auc: 0.995891\n",
      "[385]\tvalid_0's auc: 0.995887\n",
      "[386]\tvalid_0's auc: 0.995891\n",
      "[387]\tvalid_0's auc: 0.99588\n",
      "[388]\tvalid_0's auc: 0.995883\n",
      "[389]\tvalid_0's auc: 0.995879\n",
      "[390]\tvalid_0's auc: 0.995876\n",
      "[391]\tvalid_0's auc: 0.995885\n",
      "[392]\tvalid_0's auc: 0.995891\n",
      "[393]\tvalid_0's auc: 0.99588\n",
      "[394]\tvalid_0's auc: 0.995883\n",
      "[395]\tvalid_0's auc: 0.995884\n",
      "[396]\tvalid_0's auc: 0.995877\n",
      "[397]\tvalid_0's auc: 0.995867\n",
      "[398]\tvalid_0's auc: 0.995873\n",
      "[399]\tvalid_0's auc: 0.995882\n",
      "[400]\tvalid_0's auc: 0.995867\n",
      "[401]\tvalid_0's auc: 0.995871\n",
      "[402]\tvalid_0's auc: 0.995876\n",
      "[403]\tvalid_0's auc: 0.995876\n",
      "[404]\tvalid_0's auc: 0.995875\n",
      "[405]\tvalid_0's auc: 0.995878\n",
      "[406]\tvalid_0's auc: 0.995877\n",
      "[407]\tvalid_0's auc: 0.995878\n",
      "[408]\tvalid_0's auc: 0.995878\n",
      "[409]\tvalid_0's auc: 0.99587\n",
      "[410]\tvalid_0's auc: 0.995865\n",
      "[411]\tvalid_0's auc: 0.995866\n",
      "[412]\tvalid_0's auc: 0.995869\n",
      "[413]\tvalid_0's auc: 0.995884\n",
      "[414]\tvalid_0's auc: 0.995888\n",
      "[415]\tvalid_0's auc: 0.99589\n",
      "[416]\tvalid_0's auc: 0.995889\n",
      "[417]\tvalid_0's auc: 0.995888\n",
      "[418]\tvalid_0's auc: 0.995901\n",
      "[419]\tvalid_0's auc: 0.995892\n",
      "[420]\tvalid_0's auc: 0.995897\n",
      "[421]\tvalid_0's auc: 0.995891\n",
      "[422]\tvalid_0's auc: 0.995899\n",
      "[423]\tvalid_0's auc: 0.995913\n",
      "[424]\tvalid_0's auc: 0.9959\n",
      "[425]\tvalid_0's auc: 0.995904\n",
      "[426]\tvalid_0's auc: 0.995903\n",
      "[427]\tvalid_0's auc: 0.995906\n",
      "[428]\tvalid_0's auc: 0.995914\n",
      "[429]\tvalid_0's auc: 0.995911\n",
      "[430]\tvalid_0's auc: 0.995909\n",
      "[431]\tvalid_0's auc: 0.995914\n",
      "[432]\tvalid_0's auc: 0.995915\n",
      "[433]\tvalid_0's auc: 0.99591\n",
      "[434]\tvalid_0's auc: 0.995903\n",
      "[435]\tvalid_0's auc: 0.995908\n",
      "[436]\tvalid_0's auc: 0.995907\n",
      "[437]\tvalid_0's auc: 0.995902\n",
      "[438]\tvalid_0's auc: 0.995889\n",
      "[439]\tvalid_0's auc: 0.995889\n",
      "[440]\tvalid_0's auc: 0.995892\n",
      "[441]\tvalid_0's auc: 0.995897\n",
      "[442]\tvalid_0's auc: 0.995892\n",
      "[443]\tvalid_0's auc: 0.995901\n",
      "[444]\tvalid_0's auc: 0.995905\n",
      "[445]\tvalid_0's auc: 0.995906\n",
      "[446]\tvalid_0's auc: 0.995899\n",
      "[447]\tvalid_0's auc: 0.995895\n",
      "[448]\tvalid_0's auc: 0.995896\n",
      "[449]\tvalid_0's auc: 0.995891\n",
      "[450]\tvalid_0's auc: 0.995895\n",
      "[451]\tvalid_0's auc: 0.995908\n",
      "[452]\tvalid_0's auc: 0.995903\n",
      "[453]\tvalid_0's auc: 0.995913\n",
      "[454]\tvalid_0's auc: 0.995913\n",
      "[455]\tvalid_0's auc: 0.995919\n",
      "[456]\tvalid_0's auc: 0.995918\n",
      "[457]\tvalid_0's auc: 0.995926\n",
      "[458]\tvalid_0's auc: 0.995937\n",
      "[459]\tvalid_0's auc: 0.995941\n",
      "[460]\tvalid_0's auc: 0.995932\n",
      "[461]\tvalid_0's auc: 0.995937\n",
      "[462]\tvalid_0's auc: 0.995941\n",
      "[463]\tvalid_0's auc: 0.99595\n",
      "[464]\tvalid_0's auc: 0.995938\n",
      "[465]\tvalid_0's auc: 0.995936\n",
      "[466]\tvalid_0's auc: 0.995935\n",
      "[467]\tvalid_0's auc: 0.995938\n",
      "[468]\tvalid_0's auc: 0.99594\n",
      "[469]\tvalid_0's auc: 0.995941\n",
      "[470]\tvalid_0's auc: 0.995941\n",
      "[471]\tvalid_0's auc: 0.99595\n",
      "[472]\tvalid_0's auc: 0.995958\n",
      "[473]\tvalid_0's auc: 0.995961\n",
      "[474]\tvalid_0's auc: 0.99595\n",
      "[475]\tvalid_0's auc: 0.995943\n",
      "[476]\tvalid_0's auc: 0.995944\n",
      "[477]\tvalid_0's auc: 0.995934\n",
      "[478]\tvalid_0's auc: 0.995923\n",
      "[479]\tvalid_0's auc: 0.99592\n",
      "[480]\tvalid_0's auc: 0.995922\n",
      "[481]\tvalid_0's auc: 0.995931\n",
      "[482]\tvalid_0's auc: 0.995928\n",
      "[483]\tvalid_0's auc: 0.995929\n",
      "[484]\tvalid_0's auc: 0.995932\n",
      "[485]\tvalid_0's auc: 0.995927\n",
      "[486]\tvalid_0's auc: 0.995927\n",
      "[487]\tvalid_0's auc: 0.995929\n",
      "[488]\tvalid_0's auc: 0.995937\n",
      "[489]\tvalid_0's auc: 0.995944\n",
      "[490]\tvalid_0's auc: 0.995932\n",
      "[491]\tvalid_0's auc: 0.995928\n",
      "[492]\tvalid_0's auc: 0.99593\n",
      "[493]\tvalid_0's auc: 0.995932\n",
      "[494]\tvalid_0's auc: 0.99593\n",
      "[495]\tvalid_0's auc: 0.995935\n",
      "[496]\tvalid_0's auc: 0.995939\n",
      "[497]\tvalid_0's auc: 0.99595\n",
      "[498]\tvalid_0's auc: 0.995948\n",
      "[499]\tvalid_0's auc: 0.995953\n",
      "[500]\tvalid_0's auc: 0.995951\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[473]\tvalid_0's auc: 0.995961\n"
     ]
    }
   ],
   "source": [
    "# 分类的情况\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_class, test_size=10000, random_state=42)\n",
    "\n",
    "train_lgb = lgb.Dataset(data=x_train, label=y_train)\n",
    "test_lgb = lgb.Dataset(data=x_test, label=y_test)\n",
    "\n",
    "param = {'max_depth': 10, 'objective': 'binary', 'num_threads': 8, 'learning_rate': 0.1, 'bagging': 0.7,\n",
    "         'feature_fraction': 0.7, 'lambda_l1': 0.1, 'lambda_l2': 0.2, 'verbose_eval': 50, 'seed': 123454,\n",
    "         'metric': ['auc']}\n",
    "bst = lgb.train(param, train_lgb, num_boost_round=500, early_stopping_rounds=100, valid_sets=[test_lgb])\n",
    "\n",
    "y_train_binary = bst.predict(x_train, num_iteration=bst.best_iteration)  # type:np.numarray\n",
    "y_pred_binary = bst.predict(x_test, num_iteration=bst.best_iteration)  # type:np.numarray\n",
    "y_pred_binary[:30]\n",
    "y_test[:30]\n",
    "res_compare=pd.DataFrame()\n",
    "res_compare['y_pred_binary'] = y_pred_binary[:30]\n",
    "res_compare['y_test'] = y_test[:30].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "res_compare.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 15015, number of used features: 10\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 0.000719\n",
      "[1]\tvalid_0's auc: 0.855239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's auc: 0.880591\n",
      "[3]\tvalid_0's auc: 0.883168\n",
      "[4]\tvalid_0's auc: 0.909729\n",
      "[5]\tvalid_0's auc: 0.924742\n",
      "[6]\tvalid_0's auc: 0.925933\n",
      "[7]\tvalid_0's auc: 0.932938\n",
      "[8]\tvalid_0's auc: 0.93782\n",
      "[9]\tvalid_0's auc: 0.946878\n",
      "[10]\tvalid_0's auc: 0.948501\n",
      "[11]\tvalid_0's auc: 0.952839\n",
      "[12]\tvalid_0's auc: 0.958991\n",
      "[13]\tvalid_0's auc: 0.960884\n",
      "[14]\tvalid_0's auc: 0.961988\n",
      "[15]\tvalid_0's auc: 0.963088\n",
      "[16]\tvalid_0's auc: 0.963937\n",
      "[17]\tvalid_0's auc: 0.964633\n",
      "[18]\tvalid_0's auc: 0.966247\n",
      "[19]\tvalid_0's auc: 0.9675\n",
      "[20]\tvalid_0's auc: 0.969696\n",
      "[21]\tvalid_0's auc: 0.971373\n",
      "[22]\tvalid_0's auc: 0.973394\n",
      "[23]\tvalid_0's auc: 0.974499\n",
      "[24]\tvalid_0's auc: 0.975613\n",
      "[25]\tvalid_0's auc: 0.977023\n",
      "[26]\tvalid_0's auc: 0.978151\n",
      "[27]\tvalid_0's auc: 0.979622\n",
      "[28]\tvalid_0's auc: 0.981066\n",
      "[29]\tvalid_0's auc: 0.981504\n",
      "[30]\tvalid_0's auc: 0.982584\n",
      "[31]\tvalid_0's auc: 0.983367\n",
      "[32]\tvalid_0's auc: 0.984006\n",
      "[33]\tvalid_0's auc: 0.984541\n",
      "[34]\tvalid_0's auc: 0.98505\n",
      "[35]\tvalid_0's auc: 0.985809\n",
      "[36]\tvalid_0's auc: 0.986327\n",
      "[37]\tvalid_0's auc: 0.986971\n",
      "[38]\tvalid_0's auc: 0.987556\n",
      "[39]\tvalid_0's auc: 0.988156\n",
      "[40]\tvalid_0's auc: 0.988693\n",
      "[41]\tvalid_0's auc: 0.989019\n",
      "[42]\tvalid_0's auc: 0.98951\n",
      "[43]\tvalid_0's auc: 0.989818\n",
      "[44]\tvalid_0's auc: 0.990245\n",
      "[45]\tvalid_0's auc: 0.990466\n",
      "[46]\tvalid_0's auc: 0.990703\n",
      "[47]\tvalid_0's auc: 0.991025\n",
      "[48]\tvalid_0's auc: 0.991455\n",
      "[49]\tvalid_0's auc: 0.991562\n",
      "[50]\tvalid_0's auc: 0.991802\n",
      "[51]\tvalid_0's auc: 0.992061\n",
      "[52]\tvalid_0's auc: 0.992364\n",
      "[53]\tvalid_0's auc: 0.99251\n",
      "[54]\tvalid_0's auc: 0.992684\n",
      "[55]\tvalid_0's auc: 0.992872\n",
      "[56]\tvalid_0's auc: 0.993115\n",
      "[57]\tvalid_0's auc: 0.993346\n",
      "[58]\tvalid_0's auc: 0.993563\n",
      "[59]\tvalid_0's auc: 0.993702\n",
      "[60]\tvalid_0's auc: 0.993818\n",
      "[61]\tvalid_0's auc: 0.993904\n",
      "[62]\tvalid_0's auc: 0.994018\n",
      "[63]\tvalid_0's auc: 0.994157\n",
      "[64]\tvalid_0's auc: 0.994307\n",
      "[65]\tvalid_0's auc: 0.994414\n",
      "[66]\tvalid_0's auc: 0.994512\n",
      "[67]\tvalid_0's auc: 0.994595\n",
      "[68]\tvalid_0's auc: 0.994676\n",
      "[69]\tvalid_0's auc: 0.994722\n",
      "[70]\tvalid_0's auc: 0.99479\n",
      "[71]\tvalid_0's auc: 0.994894\n",
      "[72]\tvalid_0's auc: 0.994996\n",
      "[73]\tvalid_0's auc: 0.995042\n",
      "[74]\tvalid_0's auc: 0.995121\n",
      "[75]\tvalid_0's auc: 0.99517\n",
      "[76]\tvalid_0's auc: 0.995223\n",
      "[77]\tvalid_0's auc: 0.995264\n",
      "[78]\tvalid_0's auc: 0.995307\n",
      "[79]\tvalid_0's auc: 0.995364\n",
      "[80]\tvalid_0's auc: 0.995417\n",
      "[81]\tvalid_0's auc: 0.995454\n",
      "[82]\tvalid_0's auc: 0.995511\n",
      "[83]\tvalid_0's auc: 0.995538\n",
      "[84]\tvalid_0's auc: 0.995586\n",
      "[85]\tvalid_0's auc: 0.995629\n",
      "[86]\tvalid_0's auc: 0.995667\n",
      "[87]\tvalid_0's auc: 0.995706\n",
      "[88]\tvalid_0's auc: 0.99573\n",
      "[89]\tvalid_0's auc: 0.99576\n",
      "[90]\tvalid_0's auc: 0.995781\n",
      "[91]\tvalid_0's auc: 0.995806\n",
      "[92]\tvalid_0's auc: 0.995841\n",
      "[93]\tvalid_0's auc: 0.995861\n",
      "[94]\tvalid_0's auc: 0.995882\n",
      "[95]\tvalid_0's auc: 0.995909\n",
      "[96]\tvalid_0's auc: 0.995928\n",
      "[97]\tvalid_0's auc: 0.995944\n",
      "[98]\tvalid_0's auc: 0.995961\n",
      "[99]\tvalid_0's auc: 0.995975\n",
      "[100]\tvalid_0's auc: 0.995986\n",
      "[101]\tvalid_0's auc: 0.996\n",
      "[102]\tvalid_0's auc: 0.99602\n",
      "[103]\tvalid_0's auc: 0.996041\n",
      "[104]\tvalid_0's auc: 0.996051\n",
      "[105]\tvalid_0's auc: 0.996067\n",
      "[106]\tvalid_0's auc: 0.99608\n",
      "[107]\tvalid_0's auc: 0.996097\n",
      "[108]\tvalid_0's auc: 0.996107\n",
      "[109]\tvalid_0's auc: 0.996124\n",
      "[110]\tvalid_0's auc: 0.996132\n",
      "[111]\tvalid_0's auc: 0.996147\n",
      "[112]\tvalid_0's auc: 0.996157\n",
      "[113]\tvalid_0's auc: 0.996172\n",
      "[114]\tvalid_0's auc: 0.996182\n",
      "[115]\tvalid_0's auc: 0.996203\n",
      "[116]\tvalid_0's auc: 0.996209\n",
      "[117]\tvalid_0's auc: 0.996217\n",
      "[118]\tvalid_0's auc: 0.996224\n",
      "[119]\tvalid_0's auc: 0.996247\n",
      "[120]\tvalid_0's auc: 0.996259\n",
      "[121]\tvalid_0's auc: 0.99627\n",
      "[122]\tvalid_0's auc: 0.996277\n",
      "[123]\tvalid_0's auc: 0.996288\n",
      "[124]\tvalid_0's auc: 0.996313\n",
      "[125]\tvalid_0's auc: 0.996316\n",
      "[126]\tvalid_0's auc: 0.996326\n",
      "[127]\tvalid_0's auc: 0.996336\n",
      "[128]\tvalid_0's auc: 0.996347\n",
      "[129]\tvalid_0's auc: 0.996356\n",
      "[130]\tvalid_0's auc: 0.996356\n",
      "[131]\tvalid_0's auc: 0.996367\n",
      "[132]\tvalid_0's auc: 0.996366\n",
      "[133]\tvalid_0's auc: 0.996373\n",
      "[134]\tvalid_0's auc: 0.996377\n",
      "[135]\tvalid_0's auc: 0.99638\n",
      "[136]\tvalid_0's auc: 0.996385\n",
      "[137]\tvalid_0's auc: 0.996404\n",
      "[138]\tvalid_0's auc: 0.996408\n",
      "[139]\tvalid_0's auc: 0.996412\n",
      "[140]\tvalid_0's auc: 0.996422\n",
      "[141]\tvalid_0's auc: 0.996425\n",
      "[142]\tvalid_0's auc: 0.99643\n",
      "[143]\tvalid_0's auc: 0.996431\n",
      "[144]\tvalid_0's auc: 0.996441\n",
      "[145]\tvalid_0's auc: 0.996441\n",
      "[146]\tvalid_0's auc: 0.996444\n",
      "[147]\tvalid_0's auc: 0.996445\n",
      "[148]\tvalid_0's auc: 0.996449\n",
      "[149]\tvalid_0's auc: 0.996453\n",
      "[150]\tvalid_0's auc: 0.996464\n",
      "[151]\tvalid_0's auc: 0.996471\n",
      "[152]\tvalid_0's auc: 0.996488\n",
      "[153]\tvalid_0's auc: 0.996494\n",
      "[154]\tvalid_0's auc: 0.996495\n",
      "[155]\tvalid_0's auc: 0.996505\n",
      "[156]\tvalid_0's auc: 0.996503\n",
      "[157]\tvalid_0's auc: 0.996505\n",
      "[158]\tvalid_0's auc: 0.996511\n",
      "[159]\tvalid_0's auc: 0.996512\n",
      "[160]\tvalid_0's auc: 0.996512\n",
      "[161]\tvalid_0's auc: 0.996518\n",
      "[162]\tvalid_0's auc: 0.996525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[163]\tvalid_0's auc: 0.996525\n",
      "[164]\tvalid_0's auc: 0.996526\n",
      "[165]\tvalid_0's auc: 0.99653\n",
      "[166]\tvalid_0's auc: 0.996534\n",
      "[167]\tvalid_0's auc: 0.996548\n",
      "[168]\tvalid_0's auc: 0.996548\n",
      "[169]\tvalid_0's auc: 0.996548\n",
      "[170]\tvalid_0's auc: 0.996551\n",
      "[171]\tvalid_0's auc: 0.996557\n",
      "[172]\tvalid_0's auc: 0.996562\n",
      "[173]\tvalid_0's auc: 0.996563\n",
      "[174]\tvalid_0's auc: 0.996569\n",
      "[175]\tvalid_0's auc: 0.996584\n",
      "[176]\tvalid_0's auc: 0.996587\n",
      "[177]\tvalid_0's auc: 0.996599\n",
      "[178]\tvalid_0's auc: 0.996604\n",
      "[179]\tvalid_0's auc: 0.996607\n",
      "[180]\tvalid_0's auc: 0.996612\n",
      "[181]\tvalid_0's auc: 0.996614\n",
      "[182]\tvalid_0's auc: 0.996615\n",
      "[183]\tvalid_0's auc: 0.996625\n",
      "[184]\tvalid_0's auc: 0.996627\n",
      "[185]\tvalid_0's auc: 0.996628\n",
      "[186]\tvalid_0's auc: 0.996629\n",
      "[187]\tvalid_0's auc: 0.99663\n",
      "[188]\tvalid_0's auc: 0.99663\n",
      "[189]\tvalid_0's auc: 0.996631\n",
      "[190]\tvalid_0's auc: 0.99663\n",
      "[191]\tvalid_0's auc: 0.99663\n",
      "[192]\tvalid_0's auc: 0.996634\n",
      "[193]\tvalid_0's auc: 0.996636\n",
      "[194]\tvalid_0's auc: 0.996638\n",
      "[195]\tvalid_0's auc: 0.996642\n",
      "[196]\tvalid_0's auc: 0.996641\n",
      "[197]\tvalid_0's auc: 0.996659\n",
      "[198]\tvalid_0's auc: 0.996666\n",
      "[199]\tvalid_0's auc: 0.996667\n",
      "[200]\tvalid_0's auc: 0.996668\n",
      "[201]\tvalid_0's auc: 0.996674\n",
      "[202]\tvalid_0's auc: 0.996682\n",
      "[203]\tvalid_0's auc: 0.99668\n",
      "[204]\tvalid_0's auc: 0.996683\n",
      "[205]\tvalid_0's auc: 0.996691\n",
      "[206]\tvalid_0's auc: 0.996694\n",
      "[207]\tvalid_0's auc: 0.996696\n",
      "[208]\tvalid_0's auc: 0.996695\n",
      "[209]\tvalid_0's auc: 0.996695\n",
      "[210]\tvalid_0's auc: 0.996692\n",
      "[211]\tvalid_0's auc: 0.996704\n",
      "[212]\tvalid_0's auc: 0.996715\n",
      "[213]\tvalid_0's auc: 0.996725\n",
      "[214]\tvalid_0's auc: 0.996724\n",
      "[215]\tvalid_0's auc: 0.996724\n",
      "[216]\tvalid_0's auc: 0.996728\n",
      "[217]\tvalid_0's auc: 0.996736\n",
      "[218]\tvalid_0's auc: 0.996748\n",
      "[219]\tvalid_0's auc: 0.996749\n",
      "[220]\tvalid_0's auc: 0.996751\n",
      "[221]\tvalid_0's auc: 0.996755\n",
      "[222]\tvalid_0's auc: 0.996763\n",
      "[223]\tvalid_0's auc: 0.996765\n",
      "[224]\tvalid_0's auc: 0.996769\n",
      "[225]\tvalid_0's auc: 0.996776\n",
      "[226]\tvalid_0's auc: 0.996784\n",
      "[227]\tvalid_0's auc: 0.996782\n",
      "[228]\tvalid_0's auc: 0.996785\n",
      "[229]\tvalid_0's auc: 0.996791\n",
      "[230]\tvalid_0's auc: 0.996792\n",
      "[231]\tvalid_0's auc: 0.996796\n",
      "[232]\tvalid_0's auc: 0.996801\n",
      "[233]\tvalid_0's auc: 0.996803\n",
      "[234]\tvalid_0's auc: 0.996806\n",
      "[235]\tvalid_0's auc: 0.996812\n",
      "[236]\tvalid_0's auc: 0.996825\n",
      "[237]\tvalid_0's auc: 0.996831\n",
      "[238]\tvalid_0's auc: 0.996829\n",
      "[239]\tvalid_0's auc: 0.996831\n",
      "[240]\tvalid_0's auc: 0.996834\n",
      "[241]\tvalid_0's auc: 0.996841\n",
      "[242]\tvalid_0's auc: 0.996847\n",
      "[243]\tvalid_0's auc: 0.996851\n",
      "[244]\tvalid_0's auc: 0.996854\n",
      "[245]\tvalid_0's auc: 0.996864\n",
      "[246]\tvalid_0's auc: 0.996865\n",
      "[247]\tvalid_0's auc: 0.996864\n",
      "[248]\tvalid_0's auc: 0.996869\n",
      "[249]\tvalid_0's auc: 0.996867\n",
      "[250]\tvalid_0's auc: 0.996875\n",
      "[251]\tvalid_0's auc: 0.996883\n",
      "[252]\tvalid_0's auc: 0.996886\n",
      "[253]\tvalid_0's auc: 0.996887\n",
      "[254]\tvalid_0's auc: 0.996887\n",
      "[255]\tvalid_0's auc: 0.996885\n",
      "[256]\tvalid_0's auc: 0.996888\n",
      "[257]\tvalid_0's auc: 0.996894\n",
      "[258]\tvalid_0's auc: 0.996899\n",
      "[259]\tvalid_0's auc: 0.996901\n",
      "[260]\tvalid_0's auc: 0.996902\n",
      "[261]\tvalid_0's auc: 0.996903\n",
      "[262]\tvalid_0's auc: 0.996903\n",
      "[263]\tvalid_0's auc: 0.996914\n",
      "[264]\tvalid_0's auc: 0.996918\n",
      "[265]\tvalid_0's auc: 0.99692\n",
      "[266]\tvalid_0's auc: 0.996926\n",
      "[267]\tvalid_0's auc: 0.996928\n",
      "[268]\tvalid_0's auc: 0.996932\n",
      "[269]\tvalid_0's auc: 0.99694\n",
      "[270]\tvalid_0's auc: 0.996941\n",
      "[271]\tvalid_0's auc: 0.996944\n",
      "[272]\tvalid_0's auc: 0.996945\n",
      "[273]\tvalid_0's auc: 0.996944\n",
      "[274]\tvalid_0's auc: 0.996947\n",
      "[275]\tvalid_0's auc: 0.996946\n",
      "[276]\tvalid_0's auc: 0.996949\n",
      "[277]\tvalid_0's auc: 0.996951\n",
      "[278]\tvalid_0's auc: 0.996957\n",
      "[279]\tvalid_0's auc: 0.996957\n",
      "[280]\tvalid_0's auc: 0.996958\n",
      "[281]\tvalid_0's auc: 0.996958\n",
      "[282]\tvalid_0's auc: 0.996959\n",
      "[283]\tvalid_0's auc: 0.996961\n",
      "[284]\tvalid_0's auc: 0.996962\n",
      "[285]\tvalid_0's auc: 0.996965\n",
      "[286]\tvalid_0's auc: 0.996967\n",
      "[287]\tvalid_0's auc: 0.996973\n",
      "[288]\tvalid_0's auc: 0.996976\n",
      "[289]\tvalid_0's auc: 0.996975\n",
      "[290]\tvalid_0's auc: 0.996978\n",
      "[291]\tvalid_0's auc: 0.99698\n",
      "[292]\tvalid_0's auc: 0.996982\n",
      "[293]\tvalid_0's auc: 0.996982\n",
      "[294]\tvalid_0's auc: 0.996985\n",
      "[295]\tvalid_0's auc: 0.996992\n",
      "[296]\tvalid_0's auc: 0.996994\n",
      "[297]\tvalid_0's auc: 0.996995\n",
      "[298]\tvalid_0's auc: 0.996998\n",
      "[299]\tvalid_0's auc: 0.997004\n",
      "[300]\tvalid_0's auc: 0.997008\n",
      "[301]\tvalid_0's auc: 0.997012\n",
      "[302]\tvalid_0's auc: 0.997015\n",
      "[303]\tvalid_0's auc: 0.997016\n",
      "[304]\tvalid_0's auc: 0.997013\n",
      "[305]\tvalid_0's auc: 0.997015\n",
      "[306]\tvalid_0's auc: 0.997015\n",
      "[307]\tvalid_0's auc: 0.997021\n",
      "[308]\tvalid_0's auc: 0.997025\n",
      "[309]\tvalid_0's auc: 0.997023\n",
      "[310]\tvalid_0's auc: 0.997023\n",
      "[311]\tvalid_0's auc: 0.997026\n",
      "[312]\tvalid_0's auc: 0.997027\n",
      "[313]\tvalid_0's auc: 0.997037\n",
      "[314]\tvalid_0's auc: 0.997043\n",
      "[315]\tvalid_0's auc: 0.997045\n",
      "[316]\tvalid_0's auc: 0.997044\n",
      "[317]\tvalid_0's auc: 0.997047\n",
      "[318]\tvalid_0's auc: 0.997051\n",
      "[319]\tvalid_0's auc: 0.99705\n",
      "[320]\tvalid_0's auc: 0.997056\n",
      "[321]\tvalid_0's auc: 0.997055\n",
      "[322]\tvalid_0's auc: 0.997056\n",
      "[323]\tvalid_0's auc: 0.997061\n",
      "[324]\tvalid_0's auc: 0.997063\n",
      "[325]\tvalid_0's auc: 0.997067\n",
      "[326]\tvalid_0's auc: 0.997068\n",
      "[327]\tvalid_0's auc: 0.99707\n",
      "[328]\tvalid_0's auc: 0.997072\n",
      "[329]\tvalid_0's auc: 0.997074\n",
      "[330]\tvalid_0's auc: 0.997075\n",
      "[331]\tvalid_0's auc: 0.997073\n",
      "[332]\tvalid_0's auc: 0.997075\n",
      "[333]\tvalid_0's auc: 0.997076\n",
      "[334]\tvalid_0's auc: 0.997076\n",
      "[335]\tvalid_0's auc: 0.997078\n",
      "[336]\tvalid_0's auc: 0.997079\n",
      "[337]\tvalid_0's auc: 0.99708\n",
      "[338]\tvalid_0's auc: 0.997084\n",
      "[339]\tvalid_0's auc: 0.997084\n",
      "[340]\tvalid_0's auc: 0.997086\n",
      "[341]\tvalid_0's auc: 0.997087\n",
      "[342]\tvalid_0's auc: 0.997088\n",
      "[343]\tvalid_0's auc: 0.997092\n",
      "[344]\tvalid_0's auc: 0.997095\n",
      "[345]\tvalid_0's auc: 0.997101\n",
      "[346]\tvalid_0's auc: 0.997103\n",
      "[347]\tvalid_0's auc: 0.997105\n",
      "[348]\tvalid_0's auc: 0.997106\n",
      "[349]\tvalid_0's auc: 0.99711\n",
      "[350]\tvalid_0's auc: 0.997112\n",
      "[351]\tvalid_0's auc: 0.997114\n",
      "[352]\tvalid_0's auc: 0.997116\n",
      "[353]\tvalid_0's auc: 0.997118\n",
      "[354]\tvalid_0's auc: 0.997119\n",
      "[355]\tvalid_0's auc: 0.997123\n",
      "[356]\tvalid_0's auc: 0.997131\n",
      "[357]\tvalid_0's auc: 0.997133\n",
      "[358]\tvalid_0's auc: 0.997136\n",
      "[359]\tvalid_0's auc: 0.997139\n",
      "[360]\tvalid_0's auc: 0.99714\n",
      "[361]\tvalid_0's auc: 0.997139\n",
      "[362]\tvalid_0's auc: 0.997138\n",
      "[363]\tvalid_0's auc: 0.997141\n",
      "[364]\tvalid_0's auc: 0.99714\n",
      "[365]\tvalid_0's auc: 0.997139\n",
      "[366]\tvalid_0's auc: 0.997139\n",
      "[367]\tvalid_0's auc: 0.997143\n",
      "[368]\tvalid_0's auc: 0.997143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369]\tvalid_0's auc: 0.997145\n",
      "[370]\tvalid_0's auc: 0.997145\n",
      "[371]\tvalid_0's auc: 0.997145\n",
      "[372]\tvalid_0's auc: 0.997145\n",
      "[373]\tvalid_0's auc: 0.997148\n",
      "[374]\tvalid_0's auc: 0.997148\n",
      "[375]\tvalid_0's auc: 0.99715\n",
      "[376]\tvalid_0's auc: 0.997151\n",
      "[377]\tvalid_0's auc: 0.997154\n",
      "[378]\tvalid_0's auc: 0.997156\n",
      "[379]\tvalid_0's auc: 0.997155\n",
      "[380]\tvalid_0's auc: 0.997156\n",
      "[381]\tvalid_0's auc: 0.997159\n",
      "[382]\tvalid_0's auc: 0.997165\n",
      "[383]\tvalid_0's auc: 0.997164\n",
      "[384]\tvalid_0's auc: 0.997165\n",
      "[385]\tvalid_0's auc: 0.997165\n",
      "[386]\tvalid_0's auc: 0.997166\n",
      "[387]\tvalid_0's auc: 0.997169\n",
      "[388]\tvalid_0's auc: 0.997172\n",
      "[389]\tvalid_0's auc: 0.997171\n",
      "[390]\tvalid_0's auc: 0.997174\n",
      "[391]\tvalid_0's auc: 0.997174\n",
      "[392]\tvalid_0's auc: 0.997179\n",
      "[393]\tvalid_0's auc: 0.997179\n",
      "[394]\tvalid_0's auc: 0.99718\n",
      "[395]\tvalid_0's auc: 0.997181\n",
      "[396]\tvalid_0's auc: 0.997181\n",
      "[397]\tvalid_0's auc: 0.997183\n",
      "[398]\tvalid_0's auc: 0.997181\n",
      "[399]\tvalid_0's auc: 0.997185\n",
      "[400]\tvalid_0's auc: 0.997187\n",
      "[401]\tvalid_0's auc: 0.997188\n",
      "[402]\tvalid_0's auc: 0.99719\n",
      "[403]\tvalid_0's auc: 0.997194\n",
      "[404]\tvalid_0's auc: 0.997198\n",
      "[405]\tvalid_0's auc: 0.997197\n",
      "[406]\tvalid_0's auc: 0.997196\n",
      "[407]\tvalid_0's auc: 0.997195\n",
      "[408]\tvalid_0's auc: 0.997197\n",
      "[409]\tvalid_0's auc: 0.9972\n",
      "[410]\tvalid_0's auc: 0.997206\n",
      "[411]\tvalid_0's auc: 0.997208\n",
      "[412]\tvalid_0's auc: 0.997213\n",
      "[413]\tvalid_0's auc: 0.997215\n",
      "[414]\tvalid_0's auc: 0.997215\n",
      "[415]\tvalid_0's auc: 0.997214\n",
      "[416]\tvalid_0's auc: 0.997215\n",
      "[417]\tvalid_0's auc: 0.997219\n",
      "[418]\tvalid_0's auc: 0.997221\n",
      "[419]\tvalid_0's auc: 0.997221\n",
      "[420]\tvalid_0's auc: 0.997222\n",
      "[421]\tvalid_0's auc: 0.997222\n",
      "[422]\tvalid_0's auc: 0.997222\n",
      "[423]\tvalid_0's auc: 0.997224\n",
      "[424]\tvalid_0's auc: 0.997221\n",
      "[425]\tvalid_0's auc: 0.997223\n",
      "[426]\tvalid_0's auc: 0.997222\n",
      "[427]\tvalid_0's auc: 0.997226\n",
      "[428]\tvalid_0's auc: 0.997229\n",
      "[429]\tvalid_0's auc: 0.99723\n",
      "[430]\tvalid_0's auc: 0.997234\n",
      "[431]\tvalid_0's auc: 0.997235\n",
      "[432]\tvalid_0's auc: 0.997235\n",
      "[433]\tvalid_0's auc: 0.997237\n",
      "[434]\tvalid_0's auc: 0.997237\n",
      "[435]\tvalid_0's auc: 0.997236\n",
      "[436]\tvalid_0's auc: 0.997239\n",
      "[437]\tvalid_0's auc: 0.997243\n",
      "[438]\tvalid_0's auc: 0.997241\n",
      "[439]\tvalid_0's auc: 0.997243\n",
      "[440]\tvalid_0's auc: 0.997245\n",
      "[441]\tvalid_0's auc: 0.997247\n",
      "[442]\tvalid_0's auc: 0.997248\n",
      "[443]\tvalid_0's auc: 0.997249\n",
      "[444]\tvalid_0's auc: 0.997252\n",
      "[445]\tvalid_0's auc: 0.997251\n",
      "[446]\tvalid_0's auc: 0.997249\n",
      "[447]\tvalid_0's auc: 0.997252\n",
      "[448]\tvalid_0's auc: 0.997254\n",
      "[449]\tvalid_0's auc: 0.997256\n",
      "[450]\tvalid_0's auc: 0.997256\n",
      "[451]\tvalid_0's auc: 0.997258\n",
      "[452]\tvalid_0's auc: 0.997258\n",
      "[453]\tvalid_0's auc: 0.997261\n",
      "[454]\tvalid_0's auc: 0.99726\n",
      "[455]\tvalid_0's auc: 0.997259\n",
      "[456]\tvalid_0's auc: 0.99726\n",
      "[457]\tvalid_0's auc: 0.997258\n",
      "[458]\tvalid_0's auc: 0.997259\n",
      "[459]\tvalid_0's auc: 0.99726\n",
      "[460]\tvalid_0's auc: 0.99726\n",
      "[461]\tvalid_0's auc: 0.997259\n",
      "[462]\tvalid_0's auc: 0.997259\n",
      "[463]\tvalid_0's auc: 0.997261\n",
      "[464]\tvalid_0's auc: 0.997261\n",
      "[465]\tvalid_0's auc: 0.997263\n",
      "[466]\tvalid_0's auc: 0.997265\n",
      "[467]\tvalid_0's auc: 0.997267\n",
      "[468]\tvalid_0's auc: 0.997269\n",
      "[469]\tvalid_0's auc: 0.997269\n",
      "[470]\tvalid_0's auc: 0.99727\n",
      "[471]\tvalid_0's auc: 0.997273\n",
      "[472]\tvalid_0's auc: 0.997274\n",
      "[473]\tvalid_0's auc: 0.997278\n",
      "[474]\tvalid_0's auc: 0.99728\n",
      "[475]\tvalid_0's auc: 0.997279\n",
      "[476]\tvalid_0's auc: 0.997279\n",
      "[477]\tvalid_0's auc: 0.997277\n",
      "[478]\tvalid_0's auc: 0.997277\n",
      "[479]\tvalid_0's auc: 0.997281\n",
      "[480]\tvalid_0's auc: 0.997279\n",
      "[481]\tvalid_0's auc: 0.997281\n",
      "[482]\tvalid_0's auc: 0.99728\n",
      "[483]\tvalid_0's auc: 0.997279\n",
      "[484]\tvalid_0's auc: 0.997279\n",
      "[485]\tvalid_0's auc: 0.99728\n",
      "[486]\tvalid_0's auc: 0.99728\n",
      "[487]\tvalid_0's auc: 0.997281\n",
      "[488]\tvalid_0's auc: 0.997279\n",
      "[489]\tvalid_0's auc: 0.997278\n",
      "[490]\tvalid_0's auc: 0.997278\n",
      "[491]\tvalid_0's auc: 0.997279\n",
      "[492]\tvalid_0's auc: 0.997283\n",
      "[493]\tvalid_0's auc: 0.997282\n",
      "[494]\tvalid_0's auc: 0.997282\n",
      "[495]\tvalid_0's auc: 0.99728\n",
      "[496]\tvalid_0's auc: 0.997283\n",
      "[497]\tvalid_0's auc: 0.997284\n",
      "[498]\tvalid_0's auc: 0.997288\n",
      "[499]\tvalid_0's auc: 0.99729\n",
      "[500]\tvalid_0's auc: 0.997291\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's auc: 0.997291\n"
     ]
    }
   ],
   "source": [
    "# 回归的情况\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_value, test_size=10000, random_state=42)\n",
    "\n",
    "train_lgb = lgb.Dataset(data=x_train, label=y_train)\n",
    "test_lgb = lgb.Dataset(data=x_test, label=y_test)\n",
    "\n",
    "param = {'max_depth': 10, 'objective': 'regression', 'num_threads': 8, 'learning_rate': 0.1, 'bagging': 0.7,\n",
    "         'feature_fraction': 0.7, 'lambda_l1': 0.1, 'lambda_l2': 0.2, 'verbose_eval': 50, 'seed': 123454,\n",
    "         'metric': ['auc']}\n",
    "bst = lgb.train(param, train_lgb, num_boost_round=500, early_stopping_rounds=100, valid_sets=[test_lgb])\n",
    "\n",
    "y_train_binary = bst.predict(x_train, num_iteration=bst.best_iteration)  # type:np.numarray\n",
    "y_pred_binary = bst.predict(x_test, num_iteration=bst.best_iteration)  # type:np.numarray\n",
    "\n",
    "y_pred_binary[:30]\n",
    "y_test[:30]\n",
    "res_compare=pd.DataFrame()\n",
    "res_compare['y_pred_binary'] = y_pred_binary[:30]\n",
    "res_compare['y_test'] = y_test[:30].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "res_compare.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
